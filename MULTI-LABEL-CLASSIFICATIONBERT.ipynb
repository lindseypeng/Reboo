{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "from bert import tokenization\n",
    "from bert import modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_VOCAB= \"/home/ubuntu/aws_share/BERT-Base/uncased_L-12_H-768_A-12/vocab.txt\"\n",
    "BERT_INIT_CHKPNT = '/home/ubuntu/aws_share/BERT-Base/uncased_L-12_H-768_A-12/bert_model.ckpt'\n",
    "BERT_CONFIG = '/home/ubuntu/aws_share/BERT-Base/uncased_L-12_H-768_A-12/bert_config.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenization.validate_case_matches_checkpoint(True,BERT_INIT_CHKPNT)\n",
    "tokenizer = tokenization.FullTokenizer(\n",
    "      vocab_file=BERT_VOCAB, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'this',\n",
       " u'here',\n",
       " u\"'\",\n",
       " u's',\n",
       " u'an',\n",
       " u'example',\n",
       " u'of',\n",
       " u'using',\n",
       " u'the',\n",
       " u'bert',\n",
       " u'token',\n",
       " u'##izer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book#</th>\n",
       "      <th>sentences</th>\n",
       "      <th>economics</th>\n",
       "      <th>history</th>\n",
       "      <th>philosophy</th>\n",
       "      <th>psychology</th>\n",
       "      <th>science</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['A new theory of how the brain constructs emo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Scientists have long supported this assumption...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Today, however,A new theory of how the brain c...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Scientists have long supported this assumption...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Today, however, the science of emotion is in t...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book#                                          sentences  economics  \\\n",
       "0      1  ['A new theory of how the brain constructs emo...        0.0   \n",
       "1      1  Scientists have long supported this assumption...        0.0   \n",
       "2      1  Today, however,A new theory of how the brain c...        0.0   \n",
       "3      1  Scientists have long supported this assumption...        0.0   \n",
       "4      1  Today, however, the science of emotion is in t...        0.0   \n",
       "\n",
       "   history  philosophy  psychology  science  technology  \n",
       "0      0.0         0.0         1.0      1.0         0.0  \n",
       "1      0.0         0.0         1.0      1.0         0.0  \n",
       "2      0.0         0.0         1.0      1.0         0.0  \n",
       "3      0.0         0.0         1.0      1.0         0.0  \n",
       "4      0.0         0.0         1.0      1.0         0.0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_path='/home/ubuntu/trainbooksummary.csv'\n",
    "dataset0=pd.read_csv(train_data_path)\n",
    "train=dataset0.drop(dataset0.columns[0], axis=1)\n",
    "train=train.drop(train.columns[1], axis=1)\n",
    "train=train.drop(train.columns[4], axis=1)\n",
    "train=train.drop(train.columns[5], axis=1)\n",
    "train=train.drop(train.columns[4], axis=1)\n",
    "\n",
    "#train=train.drop(train.columns[6:9], axis=1)\n",
    "test_data_path='/home/ubuntu/43booktestdata.csv'\n",
    "testset=pd.read_csv(test_data_path)\n",
    "test=testset.drop(testset.columns[0], axis=1)\n",
    "test2=test.drop(['booktitle','index','sentence#','nonfiction','personaldevelopment','neuroscience'], axis=1)\n",
    "#test=testset.drop(testset.columns[1], axis=1)\n",
    "#test=train\n",
    "#dataset=dataset0.drop(columns=['Unnamed:0'])\n",
    "#train = dataset\n",
    "#test = dataset\n",
    "#a=dataset['comment_text']\n",
    "test2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book#</th>\n",
       "      <th>Description</th>\n",
       "      <th>economics</th>\n",
       "      <th>history</th>\n",
       "      <th>philosophy</th>\n",
       "      <th>psychology</th>\n",
       "      <th>science</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>A Turing Award-winning computer scientist and ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>A guide to understanding the inner workings a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>100,000 years ago, at least six human species ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Since Alexis de Tocqueville, restlessness has ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>The New York Times bestselling book coauthored...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Book#                                        Description  economics  \\\n",
       "0     61  A Turing Award-winning computer scientist and ...        0.0   \n",
       "1     60   A guide to understanding the inner workings a...        0.0   \n",
       "2     59  100,000 years ago, at least six human species ...        0.0   \n",
       "3     58  Since Alexis de Tocqueville, restlessness has ...        1.0   \n",
       "4     57  The New York Times bestselling book coauthored...        0.0   \n",
       "\n",
       "   history  philosophy  psychology  science  technology  \n",
       "0      0.0         1.0         0.0      1.0         0.0  \n",
       "1      0.0         0.0         0.0      0.0         1.0  \n",
       "2      1.0         1.0         0.0      1.0         0.0  \n",
       "3      1.0         0.0         0.0      0.0         0.0  \n",
       "4      0.0         0.0         1.0      1.0         0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
      "['economics', 'history', 'philosophy', 'psychology', 'science', 'technology']\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "CLASSES=list(train.iloc[:,2:8])\n",
    "LABEL_COLUMNS = ['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "print(LABEL_COLUMNS)\n",
    "LABEL_COLUMNS = CLASSES\n",
    "print(LABEL_COLUMNS)\n",
    "print(len(LABEL_COLUMNS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids, is_real_example=True):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids,\n",
    "        self.is_real_example=is_real_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_examples(df, labels_available=True):\n",
    "    \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "    examples = []\n",
    "    for (i, row) in enumerate(df.values):\n",
    "        guid = row[0]\n",
    "        text_a = row[1]\n",
    "        if labels_available:\n",
    "            labels = row[2:]\n",
    "            #print(len(labels))\n",
    "        else:\n",
    "            labels = [0,0,0,0,0,0]##change here?\n",
    "        examples.append(\n",
    "            InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "    return examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "TRAIN_VAL_RATIO = 0.9\n",
    "LEN = train.shape[0]\n",
    "SIZE_TRAIN = int(TRAIN_VAL_RATIO*LEN)\n",
    "\n",
    "x_train = train[:SIZE_TRAIN]\n",
    "x_val = train[SIZE_TRAIN:]\n",
    "\n",
    "train_examples = create_examples(x_train)\n",
    "#x_train.head()\n",
    "print(len(train_examples))\n",
    "print(len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples,  max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        print(example.text_a)\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(int(label))\n",
    "\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll set sequences to be at most 128 tokens long.\n",
    "MAX_SEQ_LENGTH = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute train and warmup steps from batch size\n",
    "# These hyperparameters are copied from this colab notebook (https://colab.sandbox.google.com/github/tensorflow/tpu/blob/master/tools/colab/bert_finetuning_with_cloud_tpus.ipynb)\n",
    "BATCH_SIZE = 1#32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_TRAIN_EPOCHS = 1.0\n",
    "# Warmup is a period of time where hte learning rate \n",
    "# is small and gradually increases--usually helps training.\n",
    "WARMUP_PROPORTION = 0.1\n",
    "# Model configs\n",
    "SAVE_CHECKPOINTS_STEPS = 1000\n",
    "SAVE_SUMMARY_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PaddingInputExample(object):\n",
    "    \"\"\"Fake example so the num input examples is a multiple of the batch size.\n",
    "    When running eval/predict on the TPU, we need to pad the number of examples\n",
    "    to be a multiple of the batch size, because the TPU requires a fixed batch\n",
    "    size. The alternative is to drop the last batch, which is bad because it means\n",
    "    the entire output data won't be generated.\n",
    "    We use this class instead of `None` because treating `None` as padding\n",
    "    battches could cause silent errors.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "def convert_single_example(ex_index, example, max_seq_length,\n",
    "                           tokenizer):\n",
    "    \"\"\"Converts a single `InputExample` into a single `InputFeatures`.\"\"\"\n",
    "\n",
    "    if isinstance(example, PaddingInputExample):\n",
    "        return InputFeatures(\n",
    "            input_ids=[0] * max_seq_length,\n",
    "            input_mask=[0] * max_seq_length,\n",
    "            segment_ids=[0] * max_seq_length,\n",
    "            label_ids=0,\n",
    "            is_real_example=False)\n",
    "\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "\n",
    "    if tokens_b:\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[0:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids: 0     0   0   0  0     0 0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    tokens.append(\"[CLS]\")\n",
    "    segment_ids.append(0)\n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(0)\n",
    "    tokens.append(\"[SEP]\")\n",
    "    segment_ids.append(0)\n",
    "\n",
    "    if tokens_b:\n",
    "        for token in tokens_b:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(1)\n",
    "        tokens.append(\"[SEP]\")\n",
    "        segment_ids.append(1)\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    while len(input_ids) < max_seq_length:\n",
    "        input_ids.append(0)\n",
    "        input_mask.append(0)\n",
    "        segment_ids.append(0)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "\n",
    "    labels_ids = []\n",
    "    for label in example.labels:\n",
    "        labels_ids.append(int(label))\n",
    "\n",
    "\n",
    "    feature = InputFeatures(\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        segment_ids=segment_ids,\n",
    "        label_ids=labels_ids,\n",
    "        is_real_example=True)\n",
    "    return feature\n",
    "\n",
    "\n",
    "def file_based_convert_examples_to_features(\n",
    "        examples, max_seq_length, tokenizer, output_file):\n",
    "    \"\"\"Convert a set of `InputExample`s to a TFRecord file.\"\"\"\n",
    "\n",
    "    writer = tf.python_io.TFRecordWriter(output_file)\n",
    "\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        #if ex_index % 10000 == 0:\n",
    "            #tf.logging.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n",
    "\n",
    "        feature = convert_single_example(ex_index, example,\n",
    "                                         max_seq_length, tokenizer)\n",
    "\n",
    "        def create_int_feature(values):\n",
    "            f = tf.train.Feature(int64_list=tf.train.Int64List(value=list(values)))\n",
    "            return f\n",
    "\n",
    "        features = collections.OrderedDict()\n",
    "        features[\"input_ids\"] = create_int_feature(feature.input_ids)\n",
    "        features[\"input_mask\"] = create_int_feature(feature.input_mask)\n",
    "        features[\"segment_ids\"] = create_int_feature(feature.segment_ids)\n",
    "        features[\"is_real_example\"] = create_int_feature(\n",
    "            [int(feature.is_real_example)])\n",
    "        if isinstance(feature.label_ids, list):\n",
    "            label_ids = feature.label_ids\n",
    "        else:\n",
    "            label_ids = feature.label_ids[0]\n",
    "        features[\"label_ids\"] = create_int_feature(label_ids)\n",
    "\n",
    "        tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "        writer.write(tf_example.SerializeToString())\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "def file_based_input_fn_builder(input_file, seq_length, is_training,\n",
    "                                drop_remainder):\n",
    "    \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "    name_to_features = {\n",
    "        \"input_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"input_mask\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"segment_ids\": tf.FixedLenFeature([seq_length], tf.int64),\n",
    "        \"label_ids\": tf.FixedLenFeature([6], tf.int64),\n",
    "        \"is_real_example\": tf.FixedLenFeature([], tf.int64),\n",
    "    }\n",
    "\n",
    "    def _decode_record(record, name_to_features):\n",
    "        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n",
    "        example = tf.parse_single_example(record, name_to_features)\n",
    "\n",
    "        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n",
    "        # So cast all int64 to int32.\n",
    "        for name in list(example.keys()):\n",
    "            t = example[name]\n",
    "            if t.dtype == tf.int64:\n",
    "                t = tf.to_int32(t)\n",
    "            example[name] = t\n",
    "\n",
    "        return example\n",
    "\n",
    "    def input_fn(params):\n",
    "        \"\"\"The actual input function.\"\"\"\n",
    "        batch_size = params[\"batch_size\"]\n",
    "\n",
    "        # For training, we want a lot of parallel reading and shuffling.\n",
    "        # For eval, we want no shuffling and parallel reading doesn't matter.\n",
    "        d = tf.data.TFRecordDataset(input_file)\n",
    "        if is_training:\n",
    "            d = d.repeat()\n",
    "            d = d.shuffle(buffer_size=100)\n",
    "\n",
    "        d = d.apply(\n",
    "            tf.contrib.data.map_and_batch(\n",
    "                lambda record: _decode_record(record, name_to_features),\n",
    "                batch_size=batch_size,\n",
    "                drop_remainder=drop_remainder))\n",
    "\n",
    "        return d\n",
    "\n",
    "    return input_fn\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n"
     ]
    }
   ],
   "source": [
    "# Compute # train and warmup steps from batch size\n",
    "num_train_steps = int(len(train_examples) / BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "print(num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = os.path.join('/home/ubuntu', \"train.tf_record\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(train_file):\n",
    "    open(train_file, 'w').close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Running training *****\n",
      "INFO:tensorflow:  Num examples = 54\n",
      "INFO:tensorflow:  Batch size = 1\n",
      "INFO:tensorflow:  Num steps = 54\n"
     ]
    }
   ],
   "source": [
    "file_based_convert_examples_to_features(train_examples, MAX_SEQ_LENGTH, tokenizer, train_file)\n",
    "tf.logging.info(\"***** Running training *****\")\n",
    "tf.logging.info(\"  Num examples = %d\", len(train_examples))\n",
    "tf.logging.info(\"  Batch size = %d\", BATCH_SIZE)\n",
    "tf.logging.info(\"  Num steps = %d\", num_train_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_fn = file_based_input_fn_builder(\n",
    "    input_file=train_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=True,\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(bert_config, is_training, input_ids, input_mask, segment_ids,\n",
    "                 labels, num_labels, use_one_hot_embeddings):\n",
    "    \"\"\"Creates a classification model.\"\"\"\n",
    "    model = modeling.BertModel(\n",
    "        config=bert_config,\n",
    "        is_training=is_training,\n",
    "        input_ids=input_ids,\n",
    "        input_mask=input_mask,\n",
    "        token_type_ids=segment_ids,\n",
    "        use_one_hot_embeddings=use_one_hot_embeddings)\n",
    "\n",
    "    # In the demo, we are doing a simple classification task on the entire\n",
    "    # segment.\n",
    "    #\n",
    "    # If you want to use the token-level output, use model.get_sequence_output()\n",
    "    # instead.\n",
    "    output_layer = model.get_pooled_output()\n",
    "\n",
    "    hidden_size = output_layer.shape[-1].value\n",
    "\n",
    "    output_weights = tf.get_variable(\n",
    "        \"output_weights\", [num_labels, hidden_size],\n",
    "        initializer=tf.truncated_normal_initializer(stddev=0.02))\n",
    "\n",
    "    output_bias = tf.get_variable(\n",
    "        \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        if is_training:\n",
    "            # I.e., 0.1 dropout\n",
    "            output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n",
    "\n",
    "        logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n",
    "        logits = tf.nn.bias_add(logits, output_bias)\n",
    "        \n",
    "        # probabilities = tf.nn.softmax(logits, axis=-1) ### multiclass case\n",
    "        probabilities = tf.nn.sigmoid(logits)#### multi-label case\n",
    "        \n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "        tf.logging.info(\"num_labels:{};logits:{};labels:{}\".format(num_labels, logits, labels))\n",
    "        per_example_loss = tf.nn.sigmoid_cross_entropy_with_logits(labels=labels, logits=logits)\n",
    "        loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        # probabilities = tf.nn.softmax(logits, axis=-1)\n",
    "        # log_probs = tf.nn.log_softmax(logits, axis=-1)\n",
    "        #\n",
    "        # one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "        #\n",
    "        # per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n",
    "        # loss = tf.reduce_mean(per_example_loss)\n",
    "\n",
    "        return (loss, per_example_loss, logits, probabilities)\n",
    "\n",
    "\n",
    "def model_fn_builder(bert_config, num_labels, init_checkpoint, learning_rate,\n",
    "                     num_train_steps, num_warmup_steps, use_tpu,\n",
    "                     use_one_hot_embeddings):\n",
    "    \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n",
    "\n",
    "    def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n",
    "        \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n",
    "\n",
    "        #tf.logging.info(\"*** Features ***\")\n",
    "        #for name in sorted(features.keys()):\n",
    "        #    tf.logging.info(\"  name = %s, shape = %s\" % (name, features[name].shape))\n",
    "\n",
    "        input_ids = features[\"input_ids\"]\n",
    "        input_mask = features[\"input_mask\"]\n",
    "        segment_ids = features[\"segment_ids\"]\n",
    "        label_ids = features[\"label_ids\"]\n",
    "        is_real_example = None\n",
    "        if \"is_real_example\" in features:\n",
    "             is_real_example = tf.cast(features[\"is_real_example\"], dtype=tf.float32)\n",
    "        else:\n",
    "             is_real_example = tf.ones(tf.shape(label_ids), dtype=tf.float32)\n",
    "\n",
    "        is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "        (total_loss, per_example_loss, logits, probabilities) = create_model(\n",
    "            bert_config, is_training, input_ids, input_mask, segment_ids, label_ids,\n",
    "            num_labels, use_one_hot_embeddings)\n",
    "\n",
    "        tvars = tf.trainable_variables()\n",
    "        initialized_variable_names = {}\n",
    "        scaffold_fn = None\n",
    "        if init_checkpoint:\n",
    "            (assignment_map, initialized_variable_names\n",
    "             ) = modeling.get_assignment_map_from_checkpoint(tvars, init_checkpoint)\n",
    "            if use_tpu:\n",
    "\n",
    "                def tpu_scaffold():\n",
    "                    tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "                    return tf.train.Scaffold()\n",
    "\n",
    "                scaffold_fn = tpu_scaffold\n",
    "            else:\n",
    "                tf.train.init_from_checkpoint(init_checkpoint, assignment_map)\n",
    "\n",
    "        tf.logging.info(\"**** Trainable Variables ****\")\n",
    "        for var in tvars:\n",
    "            init_string = \"\"\n",
    "            if var.name in initialized_variable_names:\n",
    "                init_string = \", *INIT_FROM_CKPT*\"\n",
    "            #tf.logging.info(\"  name = %s, shape = %s%s\", var.name, var.shape,init_string)\n",
    "\n",
    "        output_spec = None\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "\n",
    "            train_op = optimization.create_optimizer(\n",
    "                total_loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu)\n",
    "\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                train_op=train_op,\n",
    "                scaffold=scaffold_fn)\n",
    "        elif mode == tf.estimator.ModeKeys.EVAL:\n",
    "\n",
    "            def metric_fn(per_example_loss, label_ids, probabilities, is_real_example):\n",
    "\n",
    "                logits_split = tf.split(probabilities, num_labels, axis=-1)\n",
    "                label_ids_split = tf.split(label_ids, num_labels, axis=-1)\n",
    "                # metrics change to auc of every class\n",
    "                eval_dict = {}\n",
    "                for j, logits in enumerate(logits_split):\n",
    "                    label_id_ = tf.cast(label_ids_split[j], dtype=tf.int32)\n",
    "                    current_auc, update_op_auc = tf.metrics.auc(label_id_, logits)\n",
    "                    eval_dict[str(j)] = (current_auc, update_op_auc)\n",
    "                eval_dict['eval_loss'] = tf.metrics.mean(values=per_example_loss)\n",
    "                return eval_dict\n",
    "\n",
    "                ## original eval metrics\n",
    "                # predictions = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "                # accuracy = tf.metrics.accuracy(\n",
    "                #     labels=label_ids, predictions=predictions, weights=is_real_example)\n",
    "                # loss = tf.metrics.mean(values=per_example_loss, weights=is_real_example)\n",
    "                # return {\n",
    "                #     \"eval_accuracy\": accuracy,\n",
    "                #     \"eval_loss\": loss,\n",
    "                # }\n",
    "\n",
    "            eval_metrics = metric_fn(per_example_loss, label_ids, probabilities, is_real_example)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                loss=total_loss,\n",
    "                eval_metric_ops=eval_metrics,\n",
    "                scaffold=scaffold_fn)\n",
    "        else:\n",
    "            print(\"mode:\", mode,\"probabilities:\", probabilities)\n",
    "            output_spec = tf.estimator.EstimatorSpec(\n",
    "                mode=mode,\n",
    "                predictions={\"probabilities\": probabilities},\n",
    "                scaffold=scaffold_fn)\n",
    "        return output_spec\n",
    "\n",
    "    return model_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"/home/ubuntu/output\"\n",
    "# Specify outpit directory and number of checkpoint steps to save\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    model_dir=OUTPUT_DIR,\n",
    "    save_summary_steps=SAVE_SUMMARY_STEPS,\n",
    "    keep_checkpoint_max=1,\n",
    "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_task_type': 'worker', '_train_distribute': None, '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f8dedaa4250>, '_model_dir': '/home/ubuntu/output', '_protocol': None, '_save_checkpoints_steps': 1000, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_num_ps_replicas': 0, '_tf_random_seed': None, '_save_summary_steps': 500, '_device_fn': None, '_experimental_distribute': None, '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_evaluation_master': '', '_eval_distribute': None, '_global_id_in_cluster': 0, '_master': ''}\n"
     ]
    }
   ],
   "source": [
    "bert_config = modeling.BertConfig.from_json_file(BERT_CONFIG)\n",
    "model_fn = model_fn_builder(\n",
    "  bert_config=bert_config,\n",
    "  num_labels= len(LABEL_COLUMNS),\n",
    "  init_checkpoint=BERT_INIT_CHKPNT,\n",
    "  learning_rate=LEARNING_RATE,\n",
    "  num_train_steps=num_train_steps,\n",
    "  num_warmup_steps=num_warmup_steps,\n",
    "  use_tpu=False,\n",
    "  use_one_hot_embeddings=False)\n",
    "\n",
    "estimator = tf.estimator.Estimator(\n",
    "  model_fn=model_fn,\n",
    "  config=run_config,\n",
    "  params={\"batch_size\": BATCH_SIZE})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Training!\n",
      "INFO:tensorflow:Skipping training since max_steps has already saved.\n",
      "('Training took time ', datetime.timedelta(0, 0, 8635))\n"
     ]
    }
   ],
   "source": [
    "print('Beginning Training!')\n",
    "current_time = datetime.now()\n",
    "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "print(\"Training took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_file = os.path.join('/home/ubuntu', \"eval.tf_record\")\n",
    "#filename = Path(train_file)\n",
    "if not os.path.exists(eval_file):\n",
    "    open(eval_file, 'w').close()\n",
    "\n",
    "eval_examples = create_examples(x_val)\n",
    "file_based_convert_examples_to_features(\n",
    "    eval_examples, MAX_SEQ_LENGTH, tokenizer, eval_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-18-3211b50b539d>:179: map_and_batch (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.experimental.map_and_batch(...)`.\n",
      "WARNING:tensorflow:From <ipython-input-18-3211b50b539d>:159: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/bert/modeling.py:671: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "INFO:tensorflow:num_labels:6;logits:Tensor(\"loss/BiasAdd:0\", shape=(?, 6), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(?, 6), dtype=float32)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/metrics_impl.py:526: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/ops/metrics_impl.py:788: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-05-31T01:48:33Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow_p27/lib/python2.7/site-packages/tensorflow/python/training/saver.py:1272: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/output/model.ckpt-54\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-05-31-01:48:36\n",
      "INFO:tensorflow:Saving dict for global step 54: 0 = 0.9999999, 1 = 0.5833337, 2 = 0.6000001, 3 = 0.0, 4 = 0.0, 5 = 0.9999999, eval_loss = 0.3873173, global_step = 54, loss = 0.38731724\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 54: /home/ubuntu/output/model.ckpt-54\n"
     ]
    }
   ],
   "source": [
    "# This tells the estimator to run through the entire set.\n",
    "eval_steps = None\n",
    "\n",
    "eval_drop_remainder = False\n",
    "eval_input_fn = file_based_input_fn_builder(\n",
    "    input_file=eval_file,\n",
    "    seq_length=MAX_SEQ_LENGTH,\n",
    "    is_training=False,\n",
    "    drop_remainder=False)\n",
    "\n",
    "result = estimator.evaluate(input_fn=eval_input_fn, steps=eval_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:***** Eval results *****\n",
      "INFO:tensorflow:  0 = 0.9999999\n",
      "INFO:tensorflow:  1 = 0.5833337\n",
      "INFO:tensorflow:  2 = 0.6000001\n",
      "INFO:tensorflow:  3 = 0.0\n",
      "INFO:tensorflow:  4 = 0.0\n",
      "INFO:tensorflow:  5 = 0.9999999\n",
      "INFO:tensorflow:  eval_loss = 0.3873173\n",
      "INFO:tensorflow:  global_step = 54\n",
      "INFO:tensorflow:  loss = 0.38731724\n"
     ]
    }
   ],
   "source": [
    "output_eval_file = os.path.join(\"/home/ubuntu\", \"eval_results.txt\")\n",
    "with tf.gfile.GFile(output_eval_file, \"w\") as writer:\n",
    "    tf.logging.info(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        tf.logging.info(\"  %s = %s\", key, str(result[key]))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test2\n",
    "x_test = x_test.reset_index(drop=True)\n",
    "predict_examples = create_examples(x_test,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A new theory of how the brain constructs emotions that could revolutionize psychology, health care, law enforcement, and our understanding of the human mind.Emotions feel automatic, like uncontrollable reactions to things we think and experience\n",
      "Scientists have long supported this assumption by claiming that emotions are hardwired in the body or the brain\n",
      "Today, however,A new theory of how the brain constructs emotions that could revolutionize psychology, health care, law enforcement, and our understanding of the human mind.Emotions feel automatic, like uncontrollable reactions to things we think and experience\n",
      "Scientists have long supported this assumption by claiming that emotions are hardwired in the body or the brain\n",
      "Today, however, the science of emotion is in the midst of a revolution on par with the discovery of relativity in physics and natural selection in biologyâ€”ans this paradigm shift has far-reaching implications for us all.Leading the charge is psychologist and neuroscientist Lisa Feldman Barrett, whose theory of emotion is driving a deeper understanding of the mind and brain, and shedding new light on what it means to be human\n",
      "Her research overturns the widely held belief that emotions are housed in different parts of the brain and are universally expressed and recognized\n",
      "Instead, she has shown that emotion is constructed in the moment, by core systems that interact across the whole brain, aided by a lifetime of learning\n",
      "This new theory means that you play a much greater role in your emotional life than you ever thought\n",
      "['Dr\n",
      "John Bargh, the worldâ€™s leading expert on the unconscious mind, presents a â€œbrilliant and convincing bookâ€ (Malcolm Gladwell) cited as an outstanding read of 2017 by Business Insider and The Financial Timesâ€”giving us an entirely new understanding of the hidden mental processes that secretly govern every aspect of our behavior.For more than three decades, Dr\n",
      "John BarghDr\n",
      "John Bargh, the worldâ€™s leading expert on the unconscious mind, presents a â€œbrilliant and convincing bookâ€ (Malcolm Gladwell) cited as an outstanding read of 2017 by Business Insider and The Financial Timesâ€”giving us an entirely new understanding of the hidden mental processes that secretly govern every aspect of our behavior.For more than three decades, Dr\n",
      "John Bargh has conducted revolutionary research into the unconscious mind, research featured in bestsellers like Blink and Thinking Fast and Slow\n",
      "Now, in what Dr\n",
      "John Gottman said was â€œthe most important and exciting book in psychology that has been written in the past twenty years,â€ Dr\n",
      "Bargh takes us on an entertaining and enlightening tour of the forces that affect everyday behavior while transforming our understanding of ourselves in profound ways\n",
      "Dr\n",
      "Bargh takes us into his labs at New York University and Yaleâ€”where he and his colleagues have discovered how the unconscious guides our behavior, goals, and motivations in areas like race relations, parenting, business, consumer behavior, and addiction\n",
      "With infectious enthusiasm he reveals what science now knows about the pervasive influence of the unconscious mind in who we choose to date or vote for, what we buy, where we live, how we perform on tests and in job interviews, and much more\n",
      "['Why do some parents refuse to vaccinate their children? Why do some people keep guns at home, despite scientific evidence of risk to their family members? And why do people use antibiotics for illnesses they cannot possibly alleviate? When it comes to health, many people insist that science is wrong, that the evidence is incomplete, and that unidentified hazards lurk everyWhy do some parents refuse to vaccinate their children? Why do some people keep guns at home, despite scientific evidence of risk to their family members? And why do people use antibiotics for illnesses they cannot possibly alleviate? When it comes to health, many people insist that science is wrong, that the evidence is incomplete, and that unidentified hazards lurk everywhere.In Denying to the Grave, Gorman and Gorman, a father-daughter team, explore the psychology of health science denial\n",
      "Using several examples of such denial as test cases, they propose six key principles that may lead individuals to reject \"accepted\" health-related wisdom: the charismatic leader; fear of complexity; confirmation bias and the internet; fear of corporate and government conspiracies; causality and filling the ignorance gap; and the nature of risk prediction\n",
      "The authors argue that the health sciences are especially vulnerable to our innate resistance to integrate new concepts with pre-existing beliefs\n",
      "This psychological difficulty of incorporating new information is on the cutting edge of neuroscience research, as scientists continue to identify brain responses to new information that reveal deep-seated, innate discomfort with changing our minds.Denying to the Grave explores risk theory and how people make decisions about what is best for them and their loved ones, in an effort to better understand how people think when faced with significant health decisions\n",
      "[\"Humans have built hugely complex societies and technologies, but most of us don't even know how a pen or a toilet works\n",
      "How have we achieved so much despite understanding so little? Cognitive scientists Steven Sloman and Philip Fernbach argue that we survive and thrive despite our mental shortcomings because we live in a rich community of knowledge\n",
      "The key to our intelliHumans have built hugely complex societies and technologies, but most of us don't even know how a pen or a toilet works\n",
      "How have we achieved so much despite understanding so little? Cognitive scientists Steven Sloman and Philip Fernbach argue that we survive and thrive despite our mental shortcomings because we live in a rich community of knowledge\n",
      "The key to our intelligence lies in the people and things around us\n",
      "We're constantly drawing on information and expertise stored outside our heads: in our bodies, our environment, our possessions, and the community with which we interact--and usually we don't even realize we're doing it\n",
      " The human mind is both brilliant and pathetic\n",
      "We have mastered fire, created democratic institutions, stood on the moon, and sequenced our genome\n",
      "And yet each of us is error prone, sometimes irrational, and often ignorant\n",
      "The fundamentally communal nature of intelligence and knowledge explains why we often assume we know more than we really do, why political opinions and false beliefs are so hard to change, and why individually oriented approaches to education and management frequently fail\n",
      "But our collaborative minds also enable us to do amazing things\n",
      "['An award-winning scientist offers a groundbreaking new understanding of the mind-body connection and its profound impact on everything from advertising to romance.The human body is not just a passive device carrying out messages sent by the brain, but rather an integral part of how we think and make decisions\n",
      "In her groundbreaking new book, Sian Beilock, author of the higAn award-winning scientist offers a groundbreaking new understanding of the mind-body connection and its profound impact on everything from advertising to romance.The human body is not just a passive device carrying out messages sent by the brain, but rather an integral part of how we think and make decisions\n",
      "In her groundbreaking new book, Sian Beilock, author of the highly acclaimed Choke, which Time magazine praised for its â€œsmart tips...in order to think clearly...and be cool under pressure,â€ draws on her own cutting-edge research to turn the conventional understanding of the mind upside down in ways that will revolutionize how we live our lives.At the heart of How the Body Knows Its Mind is the tantalizing idea that our bodies â€œhackâ€ our brains\n",
      "The way we move affects our thoughts, our decisions, and even our preferences for particular products\n",
      "Called â€œembodied cognition,â€ this new scienceâ€”of which Beilock is a foremost researcherâ€”illuminates the power of the body and its physical surroundings to shape how we think, feel, and behave\n",
      "Beilockâ€™s findings are as varied as they are surprising\n",
      "For example, pacing around the room can enhance creativity; gesturing during a speech can help ensure that you donâ€™t draw a blank; kids learn better when their bodies are part of the learning process; walking in nature boosts concentration skills; Botox users experience less depression; and much more\n",
      "['A unique trait of the human species is that our personalities, lifestyles, and worldviews are shaped by an accident of birthâ€”namely, the culture into which we are born\n",
      "It is our cultures and not our genes that determine which foods we eat, which languages we speak, which people we love and marry, and which people we kill in war\n",
      "But how did our species develop a mind thatA unique trait of the human species is that our personalities, lifestyles, and worldviews are shaped by an accident of birthâ€”namely, the culture into which we are born\n",
      "It is our cultures and not our genes that determine which foods we eat, which languages we speak, which people we love and marry, and which people we kill in war\n",
      "But how did our species develop a mind that is hardwired for cultureâ€”and why?Evolutionary biologist Mark Pagel tracks this intriguing question through the last 80,000 years of human evolution, revealing how an innate propensity to contribute and conform to the culture of our birth not only enabled human survival and progress in the past but also continues to influence our behavior today\n",
      "['What is the mind? What is the experience of the self truly made of? How does the mind differ from the brain? Though the mindâ€™s contentsâ€”its emotions, thoughts, and memoriesâ€”are often described, the essence of mind is rarely, if ever, defined.In this book, noted neuropsychiatrist and New York Times best-selling author Daniel J\n",
      "Siegel, MD, uses his characteristic sensitivitWhat is the mind? What is the experience of the self truly made of? How does the mind differ from the brain? Though the mindâ€™s contentsâ€”its emotions, thoughts, and memoriesâ€”are often described, the essence of mind is rarely, if ever, defined.In this book, noted neuropsychiatrist and New York Times best-selling author Daniel J\n",
      "Siegel, MD, uses his characteristic sensitivity and interdisciplinary background to offer a definition of the mind that illuminates the how, what, when, where, and even why of who we are, of what the mind is, and what the mindâ€™s self has the potential to become\n",
      "MIND takes the reader on a deep personal and scientific journey into consciousness, subjective experience, and information processing, uncovering the mindâ€™s self-organizational properties that emerge from both the body and the relationships we have with one another, and with the world around us\n",
      "[\"Nerves make us bomb job interviews, first dates, and SATs\n",
      "With a presentation looming at work, fear robs us of sleep for days\n",
      "It paralyzes seasoned concert musicians and freezes rookie cops in tight situations\n",
      "And yet not everyone cracks\n",
      "Soldiers keep their heads in combat; firemen rush into burning buildings; unflappable trauma doctors juggle patient after patient\n",
      "ItNerves make us bomb job interviews, first dates, and SATs\n",
      "With a presentation looming at work, fear robs us of sleep for days\n",
      "It paralyzes seasoned concert musicians and freezes rookie cops in tight situations\n",
      "And yet not everyone cracks\n",
      "Soldiers keep their heads in combat; firemen rush into burning buildings; unflappable trauma doctors juggle patient after patient\n",
      "It's not that these people feel no fear; often, in fact, they're riddled with it\n",
      "In Nerve, Taylor Clark draws upon cutting-edge science and painstaking reporting to explore the very heart of panic and poise\n",
      "[\"A new consensus is emerging among cognitive scientists: rather than possessing fixed, unchanging memories, we create new recollections each time we are called upon to remember\n",
      "As psychologist Charles Fernyhough explains, remembering is an act of narrative imagination as much as it is the product of a neurological process\n",
      "In Pieces of Light, he illuminates this compellingA new consensus is emerging among cognitive scientists: rather than possessing fixed, unchanging memories, we create new recollections each time we are called upon to remember\n",
      "As psychologist Charles Fernyhough explains, remembering is an act of narrative imagination as much as it is the product of a neurological process\n",
      "['With a 4-page full-color insert, and black-and-white illustrations throughoutWhy do some innocent kids grow up to become cold-blooded serial killers? Is bad biology partly to blame? For more than three decades Adrian Raine has been researching the biological roots of violence and establishing neurocriminology, a new field that applies neuroscience techniques to investigateWith a 4-page full-color insert, and black-and-white illustrations throughoutWhy do some innocent kids grow up to become cold-blooded serial killers? Is bad biology partly to blame? For more than three decades Adrian Raine has been researching the biological roots of violence and establishing neurocriminology, a new field that applies neuroscience techniques to investigate the causes and cures of crime\n",
      "In The Anatomy of Violence, Raine dissects the criminal mind with a fascinating, readable, and far-reaching scientific journey into the body of evidence that reveals the brain to be a key culprit in crime causation\n",
      "\\xa0Raine documents from genetic research that the seeds of sin are sown early in life, giving rise to abnormal physiological functioning that cultivates crime\n",
      "Drawing on classical case studies of well-known killers in historyâ€”including Richard Speck, Ted Kaczynski, and Henry Lee Lucasâ€”Raine illustrates how impairments to brain areas controlling our ability to experience fear, make good decisions, and feel guilt predispose us to violence\n",
      "He contends that killers can actually be coldhearted: something as simple as a low resting heart rate can give rise to violence\n",
      "But arguing that biology is not destiny, he also sketches out provocative new biosocial treatment approaches that can change the brain and prevent violence\n",
      "\\xa0Finally, Raine tackles the thorny legal and ethical dilemmas posed by his research, visualizing a futuristic brave new world where our increasing ability to identify violent offenders early in life might shape crime-prevention policies, for good and bad\n",
      "['The U.S\n",
      "dollarâ€™s dominance seems under threat\n",
      "The near collapse of the U.S\n",
      "financial system in 2008â€“2009, political paralysis that has blocked effective policymaking, and emerging competitors such as the Chinese renminbi have heightened speculation about the dollarâ€™s looming displacement as the main reserve currency\n",
      "Yet, as The Dollar Trap powerfully argues, the financThe U.S\n",
      "dollarâ€™s dominance seems under threat\n",
      "The near collapse of the U.S\n",
      "financial system in 2008â€“2009, political paralysis that has blocked effective policymaking, and emerging competitors such as the Chinese renminbi have heightened speculation about the dollarâ€™s looming displacement as the main reserve currency\n",
      "Yet, as The Dollar Trap powerfully argues, the financial crisis, a dysfunctional international monetary system, and U.S\n",
      "policies have paradoxically strengthened the dollarâ€™s importance\n",
      "Eswar Prasad examines how the dollar came to have a central role in the world economy and demonstrates that it will remain the cornerstone of global finance for the foreseeable future\n",
      "Marshaling a range of arguments and data, and drawing on the latest research, Prasad shows why it will be difficult to dislodge the dollar-centric system\n",
      "With vast amounts of foreign financial capital locked up in dollar assets, including U.S\n",
      "government securities, other countries now have a strong incentive to prevent a dollar crash\n",
      "Prasad takes the reader through key contemporary issues in international financeâ€”including the growing economic influence of emerging markets, the currency wars, the complexities of the China-U.S\n",
      "relationship, and the role of institutions like the International Monetary Fundâ€”and offers new ideas for fixing the flawed monetary system\n",
      "['Why do we think, feel, and act in ways we wished we did not? For decades, New York Times bestselling author Dr\n",
      "David A Kessler has studied this question with regard to tobacco, food, and drugs\n",
      "Over the course of these investigations, he identified one underlying mechanism common to a broad range of human suffering\n",
      "This phenomenonâ€”captureâ€”is the process by which our atteWhy do we think, feel, and act in ways we wished we did not? For decades, New York Times bestselling author Dr\n",
      "David A Kessler has studied this question with regard to tobacco, food, and drugs\n",
      "Over the course of these investigations, he identified one underlying mechanism common to a broad range of human suffering\n",
      "This phenomenonâ€”captureâ€”is the process by which our attention is hijacked and our brains commandeered by forces outside our control.In Capture, Dr\n",
      "Kessler considers some of the most profound questions we face as human beings: What are the origins of mental afflictions, from everyday unhappiness to addiction and depressionâ€”and how are they connected? Where does healing and transcendence fit into this realm of emotional experience?Analyzing an array of insights from psychology, medicine, neuroscience, literature, philosophy, and theology, Dr\n",
      "Kessler deconstructs centuries of thinking, examining the central role of capture in mental illness and questioning traditional labels that have obscured our understanding of it\n",
      "With a new basis for understanding the phenomenon of capture, he explores the concept through the emotionally resonant stories of both well-known and un-known people caught in its throes.The closer we can come to fully comprehending the nature of capture, Dr\n",
      "Kessler argues, the better the chance to alleviate its deleterious effects and successfully change our thoughts and behavior Ultimately, Capture offers insight into how we form thoughts and emotions, manage trauma, and heal\n",
      "For the first time, we can begin to understand the underpinnings of not only mental illness, but also our everyday worries and anxieties\n",
      "['A timely and uniquely compelling plea for the importance of nurture in the ongoing nature-nurture debate.In this era of genome projects and brain scans, it is all too easy to overestimate the role of biology in human psychology\n",
      "But in this passionate corrective to the idea that DNA is destiny, Jesse Prinz focuses on the most extraordinary aspect of human nature: that nurtA timely and uniquely compelling plea for the importance of nurture in the ongoing nature-nurture debate.In this era of genome projects and brain scans, it is all too easy to overestimate the role of biology in human psychology\n",
      "But in this passionate corrective to the idea that DNA is destiny, Jesse Prinz focuses on the most extraordinary aspect of human nature: that nurture can supplement and supplant nature, allowing our minds to be profoundly influenced by experience and culture\n",
      "Drawing on cutting-edge research in neuroscience, psychology, and anthropology, Prinz shatters the myth of human uniformity and reveals how our differing cultures and life experiences make each of us unique\n",
      "Along the way he shows that we can t blame mental illness or addiction on our genes, and that societal factors shape gender differences in cognitive ability and sexual behavior\n",
      "['How is it that thoroughly physical material beings such as ourselves can think, dream, feel, create and understand ideas, theories and concepts? How does mere matter give rise to all these non-material mental states, including consciousness itself? An answer to this central question of our existence is emerging at the busy intersection of neuroscience, psychology, artificiHow is it that thoroughly physical material beings such as ourselves can think, dream, feel, create and understand ideas, theories and concepts? How does mere matter give rise to all these non-material mental states, including consciousness itself? An answer to this central question of our existence is emerging at the busy intersection of neuroscience, psychology, artificial intelligence, and robotics.In this groundbreaking work, philosopher and cognitive scientist Andy Clark explores exciting new theories from these fields that reveal minds like ours to be prediction machines - devices that have evolved to anticipate the incoming streams of sensory stimulation before they arrive\n",
      "These predictions then initiate actions that structure our worlds and alter the very things we need to engage and predict\n",
      "['You are a mind reader, born with an extraordinary ability to understand what others think, feel, believe, want, and know\n",
      "Itâ€™s a sixth sense you use every day, in every personal and professional relationship you have\n",
      "At its best, this ability allows you to achieve the most important goal in almost any life: connecting, deeply and intimately and honestly, to other human beYou are a mind reader, born with an extraordinary ability to understand what others think, feel, believe, want, and know\n",
      "Itâ€™s a sixth sense you use every day, in every personal and professional relationship you have\n",
      "At its best, this ability allows you to achieve the most important goal in almost any life: connecting, deeply and intimately and honestly, to other human beings\n",
      "At its worst, it is a source of misunderstanding and unnecessary conflict, leading to damaged relationships and broken dreams\n",
      "How good are you at knowing the minds of others? How well can you guess what others think of you, know who really likes you, or tell when someone is lying? How well do you really understand the minds of those closest to you, from your spouse to your kids to your best friends? Do you really know what your coworkers, employees, competitors, or clients want?In this illuminating exploration of one of the great mysteries of the human mind, University of Chicago psychologist Nicholas Epley introduces us to what scientists have learned about our ability to understand the most complicated puzzle on the planetâ€”other peopleâ€”and the surprising mistakes we so routinely make\n",
      "['The Executive Brain is the first book to explore in popular scientific terms one of the most important and rapidly evolving topics in contemporary neuropsychology, the most \"human\" and recently evolved region of the brain--the frontal lobes\n",
      "Crucial for all high-order functioning, it is only in humans that the frontal lobes are so highly developed\n",
      "They hold the key to ourThe Executive Brain is the first book to explore in popular scientific terms one of the most important and rapidly evolving topics in contemporary neuropsychology, the most \"human\" and recently evolved region of the brain--the frontal lobes\n",
      "Crucial for all high-order functioning, it is only in humans that the frontal lobes are so highly developed\n",
      "They hold the key to our judgment, our social and ethical behavior, our imagination, indeed, to our \"soul.\" The author shows how the frontal lobes enable us to engage in complex mental processes, how vulnerable they are to injury, and how devastating the effects of damage often are, leading to chaotic, disorganized, asocial, and even criminal behavior\n",
      " Made up of fascinating case histories and anecdotes, Goldberg\\'s book offers a panorama of state-of-the-art ideas and advances in cognitive neuroscience\n",
      "It is also an intellectual memoir, filled with vignettes about the author\\'s early training with the great Russian neuropsychologist A.R\n",
      "['â€œTimeâ€ is the most commonly used noun in the English language; itâ€™s always on our minds and it advances through every living moment\n",
      "But what is time, exactly? Do children experience it the same way adults do? Why does it seem to slow down when weâ€™re bored and speed by as we get older? How and why does time fly?In this witty and meditative exploration, award-winning authorâ€œTimeâ€ is the most commonly used noun in the English language; itâ€™s always on our minds and it advances through every living moment\n",
      "But what is time, exactly? Do children experience it the same way adults do? Why does it seem to slow down when weâ€™re bored and speed by as we get older? How and why does time fly?In this witty and meditative exploration, award-winning author and New Yorker staff writer Alan Burdick takes readers on a personal quest to understand how time gets in us and why we perceive it the way we do\n",
      "In the company of scientists, he visits the most accurate clock in the world (which exists only on paper); discovers that â€œnowâ€ actually happened a split-second ago; finds a twenty-fifth hour in the day; lives in the Arctic to lose all sense of time; and, for one fleeting moment in a neuroscientistâ€™s lab, even makes time go backward\n",
      "[\"Including a chapter by 2014 Nobel laureates May-Britt Moser and Edvard MoserAn unprecedented look at the quest to unravel the mysteries of the human brain, The Future of the Brain takes readers to the absolute frontiers of science\n",
      "Original essays by leading researchers such as Christof Koch, George Church, Olaf Sporns, and May-Britt and Edvard Moser describe the spectaculIncluding a chapter by 2014 Nobel laureates May-Britt Moser and Edvard MoserAn unprecedented look at the quest to unravel the mysteries of the human brain, The Future of the Brain takes readers to the absolute frontiers of science\n",
      "Original essays by leading researchers such as Christof Koch, George Church, Olaf Sporns, and May-Britt and Edvard Moser describe the spectacular technological advances that will enable us to map the more than eighty-five billion neurons in the brain, as well as the challenges that lie ahead in understanding the anticipated deluge of data and the prospects for building working simulations of the human brain\n",
      "['An introduction to computational thinking that traces a genealogy beginning centuries before the digital computer.A few decades into the digital era, scientists discovered that thinking in terms of computation made possible an entirely new way of organizing scientific investigation; eventually, every field had a computational branch: computational physics, computational biAn introduction to computational thinking that traces a genealogy beginning centuries before the digital computer.A few decades into the digital era, scientists discovered that thinking in terms of computation made possible an entirely new way of organizing scientific investigation; eventually, every field had a computational branch: computational physics, computational biology, computational sociology\n",
      "More recently, \"computational thinking\" has become part of the K-12 curriculum\n",
      "But what is computational thinking? This volume in the MIT Press Essential Knowledge series offers an accessible overview, tracing a genealogy that begins centuries before digital computers and portraying computational thinking as pioneers of computing have described it.The authors explain that computational thinking (CT) is not a set of concepts for programming; it is a way of thinking that is honed through practice: the mental skills for designing computations to do jobs for us, and for explaining and interpreting the world as a complex of information processes\n",
      "Mathematically trained experts (known as \"computers\") who performed complex calculations as teams engaged in CT long before electronic computers\n",
      "The authors identify six dimensions of today\\'s highly developed CT--methods, machines, computing education, software engineering, computational science, and design--and cover each in a chapter\n",
      "['It is widely understood that Charles Darwin\\'s theory of evolution completely revolutionized the study of biology\n",
      "Yet, according to David Sloan Wilson, the Darwinian revolution won\\'t be truly complete until it is applied more broadly--to everything associated with the words \"human,\" \"culture,\" and \"policy.\"In a series of engaging and insightful examples--from the breedingIt is widely understood that Charles Darwin\\'s theory of evolution completely revolutionized the study of biology\n",
      "Yet, according to David Sloan Wilson, the Darwinian revolution won\\'t be truly complete until it is applied more broadly--to everything associated with the words \"human,\" \"culture,\" and \"policy.\"In a series of engaging and insightful examples--from the breeding of hens to the timing of cataract surgeries to the organization of an automobile plant--Wilson shows how an evolutionary worldview provides a practical tool kit for understanding not only genetic evolution but also the fast-paced changes that are having an impact on our world and ourselves\n",
      "['Since the end of the second World War, economics professors and classroom textbooks have been telling us that the economy is one big machine that can be effectively regulated by economic experts and tuned by government agencies like the Federal Reserve Board\n",
      "It turns out they were wrong\n",
      "Their equations do not hold up\n",
      "Their policies have not produced the promised resultsSince the end of the second World War, economics professors and classroom textbooks have been telling us that the economy is one big machine that can be effectively regulated by economic experts and tuned by government agencies like the Federal Reserve Board\n",
      "It turns out they were wrong\n",
      "Their equations do not hold up\n",
      "Their policies have not produced the promised results\n",
      "Their interpretations of economic events -- as reported by the media -- are often of-the-mark, and unconvincing\n",
      "A key alternative to the one big machine mindset is to recognize how the economy is instead an evolutionary system, with constantly-changing patterns of specialization and trade\n",
      "This book introduces you to this powerful approach for understanding economic performance\n",
      "By putting specialization at the center of economic analysis, Arnold Kling provides you with new ways to think about issues like sustainability, financial instability, job creation, and inflation\n",
      "[\"How humans and technology evolve together in a creative partnership.In this book, Edward Ashford Lee makes a bold claim: that the creators of digital technology have an unsurpassed medium for creativity\n",
      "Technology has advanced to the point where progress seems limited not by physical constraints but the human imagination\n",
      "Writing for both literate technologists and numeraHow humans and technology evolve together in a creative partnership.In this book, Edward Ashford Lee makes a bold claim: that the creators of digital technology have an unsurpassed medium for creativity\n",
      "Technology has advanced to the point where progress seems limited not by physical constraints but the human imagination\n",
      "Writing for both literate technologists and numerate humanists, Lee makes a case for engineering--creating technology--as a deeply intellectual and fundamentally creative process\n",
      "Explaining why digital technology has been so transformative and so liberating, Lee argues that the real power of technology stems from its partnership with humans.Lee explores the ways that engineers use models and abstraction to build inventive artificial worlds and to give us things that we never dreamed of--for example, the ability to carry in our pockets everything humans have ever published\n",
      "But he also attempts to counter the runaway enthusiasm of some technology boosters who claim everything in the physical world is a computation--that even such complex phenomena as human cognition are software operating on digital data\n",
      "Lee argues that the evidence for this is weak, and the likelihood that nature has limited itself to processes that conform to today's notion of digital computation is remote.Lee goes on to argue that artificial intelligence's goal of reproducing human cognitive functions in computers vastly underestimates the potential of computers\n",
      "In his view, technology is coevolving with humans\n",
      "It augments our cognitive and physical capabilities while we nurture, develop, and propagate the technology itself\n",
      "['In this landmark book, Scott Page redefines the way we understand ourselves in relation to one another\n",
      "The Difference is about how we think in groups--and how our collective wisdom exceeds the sum of its parts\n",
      "Why can teams of people find better solutions than brilliant individuals working alone? And why are the best group decisions and predictions those that draw upon tIn this landmark book, Scott Page redefines the way we understand ourselves in relation to one another\n",
      "The Difference is about how we think in groups--and how our collective wisdom exceeds the sum of its parts\n",
      "Why can teams of people find better solutions than brilliant individuals working alone? And why are the best group decisions and predictions those that draw upon the very qualities that make each of us unique? The answers lie in diversity--not what we look like outside, but what we look like within, our distinct tools and abilities.The Difference reveals that progress and innovation may depend less on lone thinkers with enormous IQs than on diverse people working together and capitalizing on their individuality\n",
      "Page shows how groups that display a range of perspectives outperform groups of like-minded experts\n",
      "Diversity yields superior outcomes, and Page proves it using his own cutting-edge research\n",
      "Moving beyond the politics that cloud standard debates about diversity, he explains why difference beats out homogeneity, whether you\\'re talking about citizens in a democracy or scientists in the laboratory\n",
      "[\"How both logical and emotional reasoning can help us live better in our post-truth worldIn a world where fake news stories change election outcomes, has rationality become futile? In The Art of Logic in an Illogical World, Eugenia Cheng throws a lifeline to readers drowning in the illogic of contemporary life\n",
      "Cheng is a mathematician, so she knows how to make an airtightHow both logical and emotional reasoning can help us live better in our post-truth worldIn a world where fake news stories change election outcomes, has rationality become futile? In The Art of Logic in an Illogical World, Eugenia Cheng throws a lifeline to readers drowning in the illogic of contemporary life\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cheng is a mathematician, so she knows how to make an airtight argument\n",
      "But even for her, logic sometimes falls prey to emotion, which is why she still fears flying and eats more cookies than she should\n",
      "If a mathematician can't be logical, what are we to do? In this book, Cheng reveals the inner workings and limitations of logic, and explains why alogic--for example, emotion--is vital to how we think and communicate\n",
      "Cheng shows us how to use logic and alogic together to navigate a world awash in bigotry, mansplaining, and manipulative memes\n",
      "[\"How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technology--and there's nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor who's helped mainstream research on how to keep AI beneficial.How canHow will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technology--and there's nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor who's helped mainstream research on how to keep AI beneficial.How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give today's kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle?What sort of future do you want? This book empowers you to join what may be the most important conversation of our time\n",
      "['With his trademark blend of political history, social science, economics, and pop culture, two-time NYT bestselling author, syndicated columnist, National Review senior editor, and American Enterprise Institute fellow Jonah Goldberg makes the timely case that America and other democracies are in peril as they lose the will to defend the values and institutions that sustainWith his trademark blend of political history, social science, economics, and pop culture, two-time NYT bestselling author, syndicated columnist, National Review senior editor, and American Enterprise Institute fellow Jonah Goldberg makes the timely case that America and other democracies are in peril as they lose the will to defend the values and institutions that sustain freedom and prosperity\n",
      "Instead we are surrendering to populism, nationalism and other forms of tribalism\n",
      "Only once in the last 250,000 years have humans stumbled upon a way to lift ourselves out of the endless cycle of poverty, hunger, and war that defines most of history--in 18th century England when we accidentally discovered the miracle of liberal democratic capitalism.As Americans we are doubly blessed that those radical ideas were written into the Constitution, laying the groundwork for our uniquely prosperous society:  - Our rights come from God not from the government\n",
      "- The government belongs to us; we do not belong to the government\n",
      "- The individual is sovereign\n",
      "We are all captains of our own souls\n",
      " - The fruits of our labors belong to us.In the last few decades, these political virtues have been turned into vices\n",
      "As we are increasingly taught to view our traditions as a system of oppression, exploitation and \"white privilege,\" the principles of liberty and the rule of law are under attack from left and right.At a moment when authoritarianism, tribalism, identity politics, nationalism, and cults of personality are rotting our democracy from within, Goldberg exposes the West\\'s suicidal tendencies on both sides of the ideological aisle\n",
      "['A lively history seen through the fifty inventions that shaped it most profoundly, by the bestselling author of The Undercover Economist and Messy.Who thought up paper money? What was the secret element that made the Gutenberg printing press possible? And what is the connection between The Da Vinci Code and the collapse of Lehman Brothers? Fifty Inventions That Shaped theA lively history seen through the fifty inventions that shaped it most profoundly, by the bestselling author of The Undercover Economist and Messy.Who thought up paper money? What was the secret element that made the Gutenberg printing press possible? And what is the connection between The Da Vinci Code and the collapse of Lehman Brothers? Fifty Inventions That Shaped the Modern Economy paints an epic picture of change in an intimate way by telling the stories of the tools, people, and ideas that had far-reaching consequences for all of us\n",
      "From the plough to artificial intelligence, from Gilletteâ€™s disposable razor to IKEAâ€™s Billy bookcase, bestselling author and Financial Times columnist Tim Harford recounts each inventionâ€™s own curious, surprising, and memorable story\n",
      "Invention by invention, Harford reflects on how we got here and where we might go next\n",
      "He lays bare often unexpected connections: how the bar code undermined family corner stores, and why the gramophone widened inequality\n",
      "In the process, he introduces characters who developed some of these inventions, profited from them, and were ruined by them, as he traces the principles that helped explain their transformative effects\n",
      "['From one of Americaâ€™s greatest minds, a journey through psychology, philosophy, and lots of meditation to show how Buddhism holds the key to moral clarity and enduring happiness.Robert Wright famously explained in The Moral Animal how evolution shaped the human brain\n",
      "The mind is designed to often delude us, he argued, about ourselves and about the world\n",
      "And it is designeFrom one of Americaâ€™s greatest minds, a journey through psychology, philosophy, and lots of meditation to show how Buddhism holds the key to moral clarity and enduring happiness.Robert Wright famously explained in The Moral Animal how evolution shaped the human brain\n",
      "The mind is designed to often delude us, he argued, about ourselves and about the world\n",
      "And it is designed to make happiness hard to sustain.But if we know our minds are rigged for anxiety, depression, anger, and greed, what do we do? Wright locates the answer in Buddhism, which figured out thousands of years ago what scientists are only discovering now\n",
      "Buddhism holds that human suffering is a result of not seeing the world clearlyâ€”and proposes that seeing the world more clearly, through meditation, will make us better, happier people.In Why Buddhism is True, Wright leads readers on a journey through psychology, philosophy, and a great many silent retreats to show how and why meditation can serve as the foundation for a spiritual life in a secular age\n",
      "At once excitingly ambitious and wittily accessible, this is the first book to combine evolutionary psychology with cutting-edge neuroscience to defend the radical claims at the heart of Buddhist philosophy\n",
      "['In this profound and lyrical book, one of our most celebrated biologists offers a sweeping examination of the relationship between the humanities and the sciences: what they offer to each other, how they can be united, and where they still fall short\n",
      "Both endeavours, Edward O\n",
      "Wilson reveals, have their roots in human creativityâ€”the defining trait of our species.ReflectinIn this profound and lyrical book, one of our most celebrated biologists offers a sweeping examination of the relationship between the humanities and the sciences: what they offer to each other, how they can be united, and where they still fall short\n",
      "Both endeavours, Edward O\n",
      "Wilson reveals, have their roots in human creativityâ€”the defining trait of our species.Reflecting on the deepest origins of language, storytelling, and art, Wilson demonstrates how creativity began not ten thousand years ago, as we have long assumed, but over one hundred thousand years ago in the Paleolithic age\n",
      "Chronicling this evolution of creativity from primate ancestors to humans, The Origins of Creativity shows how the humanities, spurred on by the invention of language, have played a largely unexamined role in defining our species\n",
      "And in doing so, Wilson explores what we can learn about human nature from a surprising range of creative endeavorsâ€”the instinct to create gardens, the use of metaphors and irony in speech, and the power of music and song.Our achievements in science and the humanities, Wilson notes, make us uniquely advanced as a species, but also give us the potential to be supremely dangerous, most worryingly in our abuse of the planet\n",
      "['When should you think that you may be able to do something unusually well?Whether youâ€™re trying to outperform in science, or in business, or just in finding good deals shopping on eBay, itâ€™s important that you have a sober understanding of your relative competencies\n",
      "The story only ends there, however, if youâ€™re fortunate enough to live in an adequate civilization.EliezerWhen should you think that you may be able to do something unusually well?Whether youâ€™re trying to outperform in science, or in business, or just in finding good deals shopping on eBay, itâ€™s important that you have a sober understanding of your relative competencies\n",
      "The story only ends there, however, if youâ€™re fortunate enough to live in an adequate civilization.Eliezer Yudkowskyâ€™s Inadequate Equilibria is a sharp and lively guidebook for anyone questioning when and how they can know better, and do better, than the status quo\n",
      "['Human beings are primates, and primates are political animals\n",
      "Our brains, therefore, are designed not just to hunt and gather, but also to help us get ahead socially, often via deception and self-deception\n",
      "But while we may be self-interested schemers, we benefit by pretending otherwise\n",
      "The less we know about our own ugly motives, the better - and thus we don\\'t like to tHuman beings are primates, and primates are political animals\n",
      "Our brains, therefore, are designed not just to hunt and gather, but also to help us get ahead socially, often via deception and self-deception\n",
      "But while we may be self-interested schemers, we benefit by pretending otherwise\n",
      "The less we know about our own ugly motives, the better - and thus we don\\'t like to talk or even think about the extent of our selfishness\n",
      "This is \"the elephant in the brain.\" Such an introspective taboo makes it hard for us to think clearly about our nature and the explanations for our behavior\n",
      "The aim of this book, then, is to confront our hidden motives directly - to track down the darker, unexamined corners of our psyches and blast them with floodlights\n",
      "Then, once everything is clearly visible, we can work to better understand ourselves: Why do we laugh? Why are artists sexy? Why do we brag about travel? Why do we prefer to speak rather than listen?Our unconscious motives drive more than just our private behavior; they also infect our venerated social institutions such as Art, School, Charity, Medicine, Politics, and Religion\n",
      "In fact, these institutions are in many ways designed to accommodate our hidden motives, to serve covert agendas alongside their \"official\" ones\n",
      "The existence of big hidden motives can upend the usual political debates, leading one to question the legitimacy of these social institutions, and of standard policies designed to favor or discourage them\n",
      "['Our Mathematical Universe is a journey to explore the mysteries uncovered by cosmology and to discover the nature of reality\n",
      "Our Big Bang, our distant future, parallel worlds, the sub-atomic and intergalactic - none of them are what they seem\n",
      "But there is a way to understand this immense strangeness - mathematics\n",
      "Seeking an answer to the fundamental puzzle of why our unOur Mathematical Universe is a journey to explore the mysteries uncovered by cosmology and to discover the nature of reality\n",
      "Our Big Bang, our distant future, parallel worlds, the sub-atomic and intergalactic - none of them are what they seem\n",
      "But there is a way to understand this immense strangeness - mathematics\n",
      "Seeking an answer to the fundamental puzzle of why our universe seems so mathematical, Tegmark proposes a radical idea: that our physical world not only is described by mathematics, but that it is mathematics\n",
      "['Foreword by Steven PinkerBlending the informed analysis of The Signal and the Noise with the instructive iconoclasm of Think Like a Freak, a fascinating, illuminating, and witty look at what the vast amounts of information now instantly available to us reveals about ourselves and our worldâ€”provided we ask the right questions.By the end of an average day in the early twentyForeword by Steven PinkerBlending the informed analysis of The Signal and the Noise with the instructive iconoclasm of Think Like a Freak, a fascinating, illuminating, and witty look at what the vast amounts of information now instantly available to us reveals about ourselves and our worldâ€”provided we ask the right questions.By the end of an average day in the early twenty-first century, human beings searching the internet will amass eight trillion gigabytes of data\n",
      "This staggering amount of informationâ€”unprecedented in historyâ€”can tell us a great deal about who we areâ€”the fears, desires, and behaviors that drive us, and the conscious and unconscious decisions we make\n",
      "From the profound to the mundane, we can gain astonishing knowledge about the human psyche that less than twenty years ago, seemed unfathomable.Everybody Lies offers fascinating, surprising, and sometimes laugh-out-loud insights into everything from economics to ethics to sports to race to sex, gender and more, all drawn from the world of big data\n",
      "What percentage of white voters didnâ€™t vote for Barack Obama because heâ€™s black? Does where you go to school effect how successful you are in life? Do parents secretly favor boy children over girls? Do violent films affect the crime rate? Can you beat the stock market? How regularly do we lie about our sex lives and whoâ€™s more self-conscious about sex, men or women?Investigating these questions and a host of others, Seth Stephens-Davidowitz offers revelations that can help us understand ourselves and our lives better\n",
      "Drawing on studies and experiments on how we really live and think, he demonstrates in fascinating and often funny ways the extent to which all the world is indeed a lab\n",
      "With conclusions ranging from strange-but-true to thought-provoking to disturbing, he explores the power of this digital truth serum and its deeper potentialâ€”revealing biases deeply embedded within us, information we can use to change our culture, and the questions weâ€™re afraid to ask that might be essential to our healthâ€”both emotional and physical\n",
      "All of us are touched by big data everyday, and its influence is multiplying\n",
      "['Yuval Noah Harari, author of the critically-acclaimed New York Times bestseller and international phenomenon Sapiens, returns with an equally original, compelling, and provocative book, turning his focus toward humanityâ€™s future, and our quest to upgrade humans into gods.Over the past century humankind has managed to do the impossible and rein in famine, plague, and war\n",
      "TYuval Noah Harari, author of the critically-acclaimed New York Times bestseller and international phenomenon Sapiens, returns with an equally original, compelling, and provocative book, turning his focus toward humanityâ€™s future, and our quest to upgrade humans into gods.Over the past century humankind has managed to do the impossible and rein in famine, plague, and war\n",
      "This may seem hard to accept, but, as Harari explains in his trademark styleâ€”thorough, yet rivetingâ€”famine, plague and war have been transformed from incomprehensible and uncontrollable forces of nature into manageable challenges\n",
      "For the first time ever, more people die from eating too much than from eating too little; more people die from old age than from infectious diseases; and more people commit suicide than are killed by soldiers, terrorists and criminals put together\n",
      "The average American is a thousand times more likely to die from binging at McDonalds than from being blown up by Al Qaeda.What then will replace famine, plague, and war at the top of the human agenda? As the self-made gods of planet earth, what destinies will we set ourselves, and which quests will we undertake? Homo Deus\\xa0explores the projects, dreams and nightmares that will shape the twenty-first centuryâ€”from overcoming death to creating artificial life\n",
      "It asks the fundamental questions: Where do we go from here? And how will we protect this fragile world from our own destructive powers? This is the next stage of evolution\n",
      "This is\\xa0Homo Deus.With the same insight and clarity that made Sapiens an international hit and a New York Times bestseller, Harari maps out our future\n",
      "[\" Geoffrey West's research centres on a quest to find unifying principles and patterns connecting everything, from cells and ecosystems to cities, social networks and businesses\n",
      "Scale addresses big, urgent questions about global sustainability, population explosion, urbanization, ageing, cancer, human lifespans and the increasing pace of life, but also encourages us to queGeoffrey West's research centres on a quest to find unifying principles and patterns connecting everything, from cells and ecosystems to cities, social networks and businesses.Scale addresses big, urgent questions about global sustainability, population explosion, urbanization, ageing, cancer, human lifespans and the increasing pace of life, but also encourages us to question the world around us\n",
      "['From the New York Times bestselling author of The Black Swan, a bold new work that challenges many of our long-held beliefs about risk and reward, politics and religion, finance and personal responsibility In his most provocative and practical book yet, one of the foremost thinkers of our time redefines what it means to understand the world, succeed in a profession, contriFrom the New York Times bestselling author of The Black Swan, a bold new work that challenges many of our long-held beliefs about risk and reward, politics and religion, finance and personal responsibility In his most provocative and practical book yet, one of the foremost thinkers of our time redefines what it means to understand the world, succeed in a profession, contribute to a fair and just society, detect nonsense, and influence others\n",
      "Citing examples ranging from Hammurabi to Seneca, Antaeus the Giant to Donald Trump, Nassim Nicholas Taleb shows how the willingness to accept oneâ€™s own risks is an essential attribute of heroes, saints, and flourishing people in all walks of life\n",
      "As always both accessible and iconoclastic, Taleb challenges long-held beliefs about the values of those who spearhead military interventions, make financial investments, and propagate religious faiths\n",
      "Among his insights:â€¢ For social justice,\\xa0focus on symmetry and risk sharing\n",
      "You cannot make profits and transfer the risks to others, as bankers and large corporations do\n",
      "You cannot get rich without owning your own risk and paying for your own losses\n",
      "Forcing skin in the game corrects this asymmetry better than thousands of laws and regulations.â€¢ Ethical rules arenâ€™t universal\n",
      "Youâ€™re part of a group larger than you, but itâ€™s still smaller than humanity in general.â€¢ Minorities, not majorities, run the world\n",
      "The world is not run by consensus but by stubborn minorities imposing their tastes and ethics on others.â€¢ You can be an intellectual yet still be an idiot\n",
      "â€œEducated philistinesâ€ have been wrong on everything from Stalinism to Iraq to low-carb diets.â€¢ Beware of complicated solutions (that someone was paid to find)\n",
      "A simple barbell can build muscle better than expensive new machines.â€¢ True religion is commitment, not just faith\n",
      "How much you believe in something is manifested only by what youâ€™re willing to risk for it.The phrase â€œskin in the gameâ€ is one we have often heard but rarely stopped to truly dissect\n",
      "It is the backbone of risk management, but itâ€™s also an astonishingly rich worldview that, as Taleb shows in this book, applies to all aspects of our lives\n",
      "As Taleb says, â€œThe symmetry of skin in the game is a simple rule thatâ€™s necessary for fairness and justice, and the ultimate BS-buster,â€ and â€œNever trust anyone who doesnâ€™t have skin in the game\n",
      "[\"In this ground-breaking book perfect for readers of The Power of Habit and Quiet, Harvard scientist Todd Rose shows how our one-size-fits-all world is actually one-size-fits-none.Each of us knows weâ€™re different\n",
      "Weâ€™re a little taller or shorter than the average, our salary is a bit higher or lower than the average, and we wonder about who it is that is buying the average-In this ground-breaking book perfect for readers of The Power of Habit and Quiet, Harvard scientist Todd Rose shows how our one-size-fits-all world is actually one-size-fits-none.Each of us knows weâ€™re different\n",
      "Weâ€™re a little taller or shorter than the average, our salary is a bit higher or lower than the average, and we wonder about who it is that is buying the average-priced home\n",
      "All around us, we think, are the average peopleâ€”with the average height, the average salary and the average house.But the average doesnâ€™t just influence how we see ourselvesâ€”our entire social system has been built around this average-size-fits-all model\n",
      "Schools are designed for the average student\n",
      "Healthcare is designed for the average patient\n",
      "Employers try to fill average job descriptions with employees on an average career trajectory\n",
      "Our government implements programs and initiatives to serve the average person\n",
      "For more than a century, weâ€™ve believed that the best way to run our institutions is by focusing on the average person\n",
      "But when you actually drill down into the numbers, you find an amazing fact: no one is averageâ€”which means that our society built for everyone is actually serving no one.In the 1950s, the American Air Force found itself with a massive problemâ€”performance in expensive, custom-made planes was suffering terribly, with crashes peaking at seventeen in a single day\n",
      "Since the state-of-the-art planes they were flying had been meticulously crafted to fit the average pilot, pilot error was assumed to be at fault\n",
      "Until, that is, the Air Force investigated just how many of their pilots were actually average\n",
      "The shocking answer: out of thousands of active-duty pilots, exactly zero were average\n",
      "Not one\n",
      "This discovery led to simple solutions (like adjustable seats) that dramatically reduced accidents, improved performance, and expanded the pool of potential pilots\n",
      "It also led to a huge change in thinking: planes didnâ€™t need to be designed for everyoneâ€”they needed to be designed so they could adapt to suit the individual flying them.The End of Average shows how success lies in customizing to our individual needs in all aspects of our lives, from the way we mark tests to the medical treatment we receive\n",
      "Using principles from The Science of the Individual, it shows how we can break down the average to create individualized success that benefits everyone in the long run\n",
      "[\"A fascinating exploration of how insights from computer algorithms can be applied to our everyday lives, helping to solve common decision-making problems and illuminate the workings of the human mindAll our lives are constrained by limited space and time, limits that give rise to a particular set of problems\n",
      "What should we do, or leave undone, in a day or a lifetime? HowA fascinating exploration of how insights from computer algorithms can be applied to our everyday lives, helping to solve common decision-making problems and illuminate the workings of the human mindAll our lives are constrained by limited space and time, limits that give rise to a particular set of problems\n",
      "What should we do, or leave undone, in a day or a lifetime? How much messiness should we accept? What balance of new activities and familiar favorites is the most fulfilling? These may seem like uniquely human quandaries, but they are not: computers, too, face the same constraints, so computer scientists have been grappling with their version of such issues for decades\n",
      "And the solutions they've found have much to teach us.In a dazzlingly interdisciplinary work, acclaimed author Brian Christian and cognitive scientist Tom Griffiths show how the algorithms used by computers can also untangle very human questions\n",
      "They explain how to have better hunches and when to leave things to chance, how to deal with overwhelming choices and how best to connect with others\n",
      "['Everyone knows that timing is everything\n",
      "But we don\\'t know much about timing itself\n",
      "Our lives are a never-ending stream of \"when\" decisions: when to start a business, schedule a class, get serious about a person\n",
      "Yet we make those decisions based on intuition and guesswork.Timing, it\\'s often assumed, is an art\n",
      "In When: The Scientific Secrets of Perfect Timing, Pink showEveryone knows that timing is everything\n",
      "But we don\\'t know much about timing itself\n",
      "Our lives are a never-ending stream of \"when\" decisions: when to start a business, schedule a class, get serious about a person\n",
      "Yet we make those decisions based on intuition and guesswork.Timing, it\\'s often assumed, is an art\n",
      "In When: The Scientific Secrets of Perfect Timing, Pink shows that timing is really a science.Drawing on a rich trove of research from psychology, biology, and economics, Pink reveals how best to live, work, and succeed\n",
      "['A Financial Times \"Business Book of the Month\" Based on his work at some of the world\\'s largest companies, including Ford, Adidas, and Chanel, Christian Madsbjerg\\'s Sensemaking is a provocative stand against the tyranny of big data and scientism, and an urgent, overdue defense of human intelligence\n",
      "Humans have become subservient to algorithms\n",
      "Every day brings a new MoneyA Financial Times \"Business Book of the Month\" Based on his work at some of the world\\'s largest companies, including Ford, Adidas, and Chanel, Christian Madsbjerg\\'s Sensemaking is a provocative stand against the tyranny of big data and scientism, and an urgent, overdue defense of human intelligence\n",
      "Humans have become subservient to algorithms\n",
      "Every day brings a new Moneyball fix--a math whiz who will crack open an industry with clean fact-based analysis rather than human intuition and experience\n",
      "As a result, we have stopped thinking\n",
      "Machines do it for us\n",
      "Christian Madsbjerg argues that our fixation with data often masks stunning deficiencies, and the risks for humankind are enormous\n",
      "Blind devotion to number crunching imperils our businesses, our educations, our governments, and our life savings\n",
      "Too many companies have lost touch with the humanity of their customers, while marginalizing workers with liberal arts-based skills\n",
      "Contrary to popular thinking, Madsbjerg shows how many of today\\'s biggest success stories stem not from \"quant\" thinking but from deep, nuanced engagement with culture, language, and history\n",
      "He calls his method sensemaking\n",
      "In this landmark book, Madsbjerg lays out five principles for how business leaders, entrepreneurs, and individuals can use it to solve their thorniest problems\n",
      "He profiles companies using sensemaking to connect with new customers, and takes readers inside the work process of sensemaking \"connoisseurs\" like investor George Soros, architect Bjarke Ingels, and others\n",
      "['A few common principles drive performance, regardless of the field or the task at hand\n",
      "Whether someone is trying to qualify for the Olympics, break ground in mathematical theory or craft an artistic masterpiece, many of the practices that lead to great success are the same\n",
      "In Peak Performance, Brad Stulberg, a former McKinsey and Company consultant and journalist who covA few common principles drive performance, regardless of the field or the task at hand\n",
      "Whether someone is trying to qualify for the Olympics, break ground in mathematical theory or craft an artistic masterpiece, many of the practices that lead to great success are the same\n",
      "In Peak Performance, Brad Stulberg, a former McKinsey and Company consultant and journalist who covers health and the science of human performance, and Steve Magness, a performance scientist and coach of Olympic athletes, team up to demystify these practices and demonstrate how everyone can achieve their best.The first book of its kind, Peak Performance combines the inspiring stories of top performers across a range of capabilities - from athletic, to intellectual, to artistic - with the latest scientific insights into the cognitive and neurochemical factors that drive performance in all domains\n",
      "In doing so, Peak Performance uncovers new linkages that hold promise as performance enhancers but have been overlooked in our traditionally-siloed ways of thinking\n",
      "The result is a life-changing book in which readers will learn how to enhance their performance by a myriad of ways including: optimally alternating between periods of intense work and rest; developing and harnessing the power of a self-transcending purpose; and priming the body and mind for enhanced productivity.In revealing the science of great performance and the stories of great performers across a wide range of capabilities, Peak Performance uncovers the secrets of success, and coaches readers on how to use them\n"
     ]
    }
   ],
   "source": [
    "test_features = convert_examples_to_features(predict_examples, MAX_SEQ_LENGTH, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn_builder(features, seq_length, is_training, drop_remainder):\n",
    "  \"\"\"Creates an `input_fn` closure to be passed to TPUEstimator.\"\"\"\n",
    "\n",
    "  all_input_ids = []\n",
    "  all_input_mask = []\n",
    "  all_segment_ids = []\n",
    "  all_label_ids = []\n",
    "\n",
    "  for feature in features:\n",
    "    all_input_ids.append(feature.input_ids)\n",
    "    all_input_mask.append(feature.input_mask)\n",
    "    all_segment_ids.append(feature.segment_ids)\n",
    "    all_label_ids.append(feature.label_ids)\n",
    "\n",
    "  def input_fn(params):\n",
    "    \"\"\"The actual input function.\"\"\"\n",
    "    batch_size = params[\"batch_size\"]\n",
    "\n",
    "    num_examples = len(features)\n",
    "\n",
    "    # This is for demo purposes and does NOT scale to large data sets. We do\n",
    "    # not use Dataset.from_generator() because that uses tf.py_func which is\n",
    "    # not TPU compatible. The right way to load data is with TFRecordReader.\n",
    "    d = tf.data.Dataset.from_tensor_slices({\n",
    "        \"input_ids\":\n",
    "            tf.constant(\n",
    "                all_input_ids, shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"input_mask\":\n",
    "            tf.constant(\n",
    "                all_input_mask,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"segment_ids\":\n",
    "            tf.constant(\n",
    "                all_segment_ids,\n",
    "                shape=[num_examples, seq_length],\n",
    "                dtype=tf.int32),\n",
    "        \"label_ids\":\n",
    "            tf.constant(all_label_ids, shape=[num_examples, len(LABEL_COLUMNS)], dtype=tf.int32),\n",
    "    })\n",
    "\n",
    "    if is_training:\n",
    "      d = d.repeat()\n",
    "      d = d.shuffle(buffer_size=100)\n",
    "\n",
    "    d = d.batch(batch_size=batch_size, drop_remainder=drop_remainder)\n",
    "    return d\n",
    "\n",
    "  return input_fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Predictions!\n",
      "('Prediction took time ', datetime.timedelta(0, 0, 1039))\n"
     ]
    }
   ],
   "source": [
    "print('Beginning Predictions!')\n",
    "current_time = datetime.now()\n",
    "predict_input_fn = input_fn_builder(features=test_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n",
    "predictions = estimator.predict(predict_input_fn)\n",
    "print(\"Prediction took time \", datetime.now() - current_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:num_labels:6;logits:Tensor(\"loss/BiasAdd:0\", shape=(?, 6), dtype=float32);labels:Tensor(\"loss/Cast:0\", shape=(?, 6), dtype=float32)\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "('mode:', 'infer', 'probabilities:', <tf.Tensor 'loss/Sigmoid:0' shape=(?, 6) dtype=float32>)\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /home/ubuntu/output/model.ckpt-54\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "def create_output(predictions):\n",
    "    probabilities = []\n",
    "    for (i, prediction) in enumerate(predictions):\n",
    "        preds = prediction[\"probabilities\"]\n",
    "        probabilities.append(preds)\n",
    "    dff = pd.DataFrame(probabilities)\n",
    "    dff.columns = LABEL_COLUMNS\n",
    "    \n",
    "    return dff\n",
    "output_df = create_output(predictions)\n",
    "#merged_df =  pd.concat([x_test, output_df], axis=1) ##only use these make sure original test data set has the same headers\n",
    "#submission = merged_df.drop(['comment_text'], axis=1)\n",
    "output_df.to_csv(\"/home/ubuntu/sample_submission0.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>economics</th>\n",
       "      <th>history</th>\n",
       "      <th>philosophy</th>\n",
       "      <th>psychology</th>\n",
       "      <th>science</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.081284</td>\n",
       "      <td>0.146199</td>\n",
       "      <td>0.387540</td>\n",
       "      <td>0.493134</td>\n",
       "      <td>0.763375</td>\n",
       "      <td>0.205233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.080960</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.406308</td>\n",
       "      <td>0.492032</td>\n",
       "      <td>0.769973</td>\n",
       "      <td>0.199691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.081747</td>\n",
       "      <td>0.144719</td>\n",
       "      <td>0.384300</td>\n",
       "      <td>0.492543</td>\n",
       "      <td>0.762927</td>\n",
       "      <td>0.205650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.080960</td>\n",
       "      <td>0.140949</td>\n",
       "      <td>0.406308</td>\n",
       "      <td>0.492032</td>\n",
       "      <td>0.769973</td>\n",
       "      <td>0.199691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.083853</td>\n",
       "      <td>0.145827</td>\n",
       "      <td>0.383698</td>\n",
       "      <td>0.495286</td>\n",
       "      <td>0.761881</td>\n",
       "      <td>0.201147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.080461</td>\n",
       "      <td>0.143867</td>\n",
       "      <td>0.396522</td>\n",
       "      <td>0.491269</td>\n",
       "      <td>0.766019</td>\n",
       "      <td>0.202851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.081425</td>\n",
       "      <td>0.142492</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.491219</td>\n",
       "      <td>0.764058</td>\n",
       "      <td>0.203064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.080905</td>\n",
       "      <td>0.135765</td>\n",
       "      <td>0.411094</td>\n",
       "      <td>0.489549</td>\n",
       "      <td>0.761681</td>\n",
       "      <td>0.214059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.096651</td>\n",
       "      <td>0.153937</td>\n",
       "      <td>0.448807</td>\n",
       "      <td>0.511434</td>\n",
       "      <td>0.756444</td>\n",
       "      <td>0.203662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.150210</td>\n",
       "      <td>0.386156</td>\n",
       "      <td>0.501674</td>\n",
       "      <td>0.761108</td>\n",
       "      <td>0.201059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.103551</td>\n",
       "      <td>0.157505</td>\n",
       "      <td>0.459648</td>\n",
       "      <td>0.506768</td>\n",
       "      <td>0.772993</td>\n",
       "      <td>0.198668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.085103</td>\n",
       "      <td>0.150210</td>\n",
       "      <td>0.386156</td>\n",
       "      <td>0.501674</td>\n",
       "      <td>0.761108</td>\n",
       "      <td>0.201059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.085193</td>\n",
       "      <td>0.143549</td>\n",
       "      <td>0.393119</td>\n",
       "      <td>0.498694</td>\n",
       "      <td>0.768680</td>\n",
       "      <td>0.197950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.083288</td>\n",
       "      <td>0.154772</td>\n",
       "      <td>0.392255</td>\n",
       "      <td>0.487045</td>\n",
       "      <td>0.767614</td>\n",
       "      <td>0.204334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.082492</td>\n",
       "      <td>0.152486</td>\n",
       "      <td>0.407379</td>\n",
       "      <td>0.501995</td>\n",
       "      <td>0.762854</td>\n",
       "      <td>0.204372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.082388</td>\n",
       "      <td>0.138947</td>\n",
       "      <td>0.395833</td>\n",
       "      <td>0.492218</td>\n",
       "      <td>0.765939</td>\n",
       "      <td>0.205163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.121694</td>\n",
       "      <td>0.160455</td>\n",
       "      <td>0.506137</td>\n",
       "      <td>0.507255</td>\n",
       "      <td>0.752281</td>\n",
       "      <td>0.214745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.083445</td>\n",
       "      <td>0.145240</td>\n",
       "      <td>0.390019</td>\n",
       "      <td>0.499667</td>\n",
       "      <td>0.768146</td>\n",
       "      <td>0.195207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.083848</td>\n",
       "      <td>0.143468</td>\n",
       "      <td>0.390914</td>\n",
       "      <td>0.493470</td>\n",
       "      <td>0.758340</td>\n",
       "      <td>0.203117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.088031</td>\n",
       "      <td>0.152013</td>\n",
       "      <td>0.370210</td>\n",
       "      <td>0.497361</td>\n",
       "      <td>0.749725</td>\n",
       "      <td>0.203368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.086992</td>\n",
       "      <td>0.155779</td>\n",
       "      <td>0.383868</td>\n",
       "      <td>0.494335</td>\n",
       "      <td>0.758061</td>\n",
       "      <td>0.196931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.081623</td>\n",
       "      <td>0.142953</td>\n",
       "      <td>0.404795</td>\n",
       "      <td>0.488019</td>\n",
       "      <td>0.764454</td>\n",
       "      <td>0.201981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.083854</td>\n",
       "      <td>0.139906</td>\n",
       "      <td>0.385254</td>\n",
       "      <td>0.494115</td>\n",
       "      <td>0.761095</td>\n",
       "      <td>0.203807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.082174</td>\n",
       "      <td>0.158297</td>\n",
       "      <td>0.393931</td>\n",
       "      <td>0.485424</td>\n",
       "      <td>0.758052</td>\n",
       "      <td>0.205951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.082964</td>\n",
       "      <td>0.143239</td>\n",
       "      <td>0.386755</td>\n",
       "      <td>0.495257</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.201933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.082129</td>\n",
       "      <td>0.142481</td>\n",
       "      <td>0.398661</td>\n",
       "      <td>0.485851</td>\n",
       "      <td>0.764020</td>\n",
       "      <td>0.204266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.082964</td>\n",
       "      <td>0.143239</td>\n",
       "      <td>0.386755</td>\n",
       "      <td>0.495257</td>\n",
       "      <td>0.757435</td>\n",
       "      <td>0.201933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.081407</td>\n",
       "      <td>0.141287</td>\n",
       "      <td>0.406771</td>\n",
       "      <td>0.491778</td>\n",
       "      <td>0.764096</td>\n",
       "      <td>0.205929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.082819</td>\n",
       "      <td>0.146104</td>\n",
       "      <td>0.391865</td>\n",
       "      <td>0.490361</td>\n",
       "      <td>0.758291</td>\n",
       "      <td>0.204744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.080485</td>\n",
       "      <td>0.140795</td>\n",
       "      <td>0.407174</td>\n",
       "      <td>0.489845</td>\n",
       "      <td>0.771014</td>\n",
       "      <td>0.204740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0.083365</td>\n",
       "      <td>0.160997</td>\n",
       "      <td>0.398397</td>\n",
       "      <td>0.487188</td>\n",
       "      <td>0.753722</td>\n",
       "      <td>0.206966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>0.085054</td>\n",
       "      <td>0.139094</td>\n",
       "      <td>0.428869</td>\n",
       "      <td>0.485480</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.218413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>0.085742</td>\n",
       "      <td>0.149268</td>\n",
       "      <td>0.386235</td>\n",
       "      <td>0.493035</td>\n",
       "      <td>0.758248</td>\n",
       "      <td>0.196935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>0.082176</td>\n",
       "      <td>0.147220</td>\n",
       "      <td>0.396642</td>\n",
       "      <td>0.491838</td>\n",
       "      <td>0.761352</td>\n",
       "      <td>0.204333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>0.083432</td>\n",
       "      <td>0.145974</td>\n",
       "      <td>0.394160</td>\n",
       "      <td>0.488314</td>\n",
       "      <td>0.756478</td>\n",
       "      <td>0.206144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>0.085054</td>\n",
       "      <td>0.139094</td>\n",
       "      <td>0.428869</td>\n",
       "      <td>0.485480</td>\n",
       "      <td>0.762975</td>\n",
       "      <td>0.218413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>0.085742</td>\n",
       "      <td>0.149268</td>\n",
       "      <td>0.386235</td>\n",
       "      <td>0.493035</td>\n",
       "      <td>0.758248</td>\n",
       "      <td>0.196935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.082176</td>\n",
       "      <td>0.147220</td>\n",
       "      <td>0.396642</td>\n",
       "      <td>0.491838</td>\n",
       "      <td>0.761352</td>\n",
       "      <td>0.204333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>0.081701</td>\n",
       "      <td>0.151798</td>\n",
       "      <td>0.392981</td>\n",
       "      <td>0.488130</td>\n",
       "      <td>0.757891</td>\n",
       "      <td>0.206170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>0.085071</td>\n",
       "      <td>0.148486</td>\n",
       "      <td>0.383551</td>\n",
       "      <td>0.498654</td>\n",
       "      <td>0.761213</td>\n",
       "      <td>0.198458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>0.083775</td>\n",
       "      <td>0.135065</td>\n",
       "      <td>0.409145</td>\n",
       "      <td>0.490356</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.204781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>0.084965</td>\n",
       "      <td>0.143742</td>\n",
       "      <td>0.386896</td>\n",
       "      <td>0.496411</td>\n",
       "      <td>0.761956</td>\n",
       "      <td>0.199775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>0.083775</td>\n",
       "      <td>0.135065</td>\n",
       "      <td>0.409145</td>\n",
       "      <td>0.490356</td>\n",
       "      <td>0.765941</td>\n",
       "      <td>0.204781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>0.082319</td>\n",
       "      <td>0.142386</td>\n",
       "      <td>0.397918</td>\n",
       "      <td>0.489065</td>\n",
       "      <td>0.762668</td>\n",
       "      <td>0.202005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>0.084283</td>\n",
       "      <td>0.141584</td>\n",
       "      <td>0.428508</td>\n",
       "      <td>0.484331</td>\n",
       "      <td>0.760022</td>\n",
       "      <td>0.210793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>0.084981</td>\n",
       "      <td>0.130835</td>\n",
       "      <td>0.412155</td>\n",
       "      <td>0.489763</td>\n",
       "      <td>0.765727</td>\n",
       "      <td>0.208824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>0.083710</td>\n",
       "      <td>0.144262</td>\n",
       "      <td>0.403361</td>\n",
       "      <td>0.490503</td>\n",
       "      <td>0.765894</td>\n",
       "      <td>0.201086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>291</th>\n",
       "      <td>0.083653</td>\n",
       "      <td>0.141793</td>\n",
       "      <td>0.399337</td>\n",
       "      <td>0.491239</td>\n",
       "      <td>0.760314</td>\n",
       "      <td>0.206270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>0.081469</td>\n",
       "      <td>0.141353</td>\n",
       "      <td>0.400527</td>\n",
       "      <td>0.488288</td>\n",
       "      <td>0.761137</td>\n",
       "      <td>0.204321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>0.083346</td>\n",
       "      <td>0.145944</td>\n",
       "      <td>0.392543</td>\n",
       "      <td>0.488315</td>\n",
       "      <td>0.759542</td>\n",
       "      <td>0.204455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>0.086446</td>\n",
       "      <td>0.142355</td>\n",
       "      <td>0.419190</td>\n",
       "      <td>0.491795</td>\n",
       "      <td>0.763711</td>\n",
       "      <td>0.208295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>0.084213</td>\n",
       "      <td>0.147191</td>\n",
       "      <td>0.400556</td>\n",
       "      <td>0.495741</td>\n",
       "      <td>0.764531</td>\n",
       "      <td>0.202379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>0.084629</td>\n",
       "      <td>0.147084</td>\n",
       "      <td>0.381661</td>\n",
       "      <td>0.499022</td>\n",
       "      <td>0.763895</td>\n",
       "      <td>0.192599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>0.083610</td>\n",
       "      <td>0.146985</td>\n",
       "      <td>0.415901</td>\n",
       "      <td>0.491329</td>\n",
       "      <td>0.757547</td>\n",
       "      <td>0.208384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.083107</td>\n",
       "      <td>0.146925</td>\n",
       "      <td>0.399797</td>\n",
       "      <td>0.489686</td>\n",
       "      <td>0.762292</td>\n",
       "      <td>0.207821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.085756</td>\n",
       "      <td>0.138976</td>\n",
       "      <td>0.398018</td>\n",
       "      <td>0.494349</td>\n",
       "      <td>0.763734</td>\n",
       "      <td>0.199436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.083107</td>\n",
       "      <td>0.146925</td>\n",
       "      <td>0.399797</td>\n",
       "      <td>0.489686</td>\n",
       "      <td>0.762292</td>\n",
       "      <td>0.207821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.085986</td>\n",
       "      <td>0.137841</td>\n",
       "      <td>0.390230</td>\n",
       "      <td>0.496191</td>\n",
       "      <td>0.761626</td>\n",
       "      <td>0.204471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.083858</td>\n",
       "      <td>0.137943</td>\n",
       "      <td>0.401072</td>\n",
       "      <td>0.489539</td>\n",
       "      <td>0.763566</td>\n",
       "      <td>0.202326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>0.084990</td>\n",
       "      <td>0.144653</td>\n",
       "      <td>0.384612</td>\n",
       "      <td>0.496984</td>\n",
       "      <td>0.761131</td>\n",
       "      <td>0.202513</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>304 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     economics   history  philosophy  psychology   science  technology\n",
       "0     0.081284  0.146199    0.387540    0.493134  0.763375    0.205233\n",
       "1     0.080960  0.140949    0.406308    0.492032  0.769973    0.199691\n",
       "2     0.081747  0.144719    0.384300    0.492543  0.762927    0.205650\n",
       "3     0.080960  0.140949    0.406308    0.492032  0.769973    0.199691\n",
       "4     0.083853  0.145827    0.383698    0.495286  0.761881    0.201147\n",
       "5     0.080461  0.143867    0.396522    0.491269  0.766019    0.202851\n",
       "6     0.081425  0.142492    0.399340    0.491219  0.764058    0.203064\n",
       "7     0.080905  0.135765    0.411094    0.489549  0.761681    0.214059\n",
       "8     0.096651  0.153937    0.448807    0.511434  0.756444    0.203662\n",
       "9     0.085103  0.150210    0.386156    0.501674  0.761108    0.201059\n",
       "10    0.103551  0.157505    0.459648    0.506768  0.772993    0.198668\n",
       "11    0.085103  0.150210    0.386156    0.501674  0.761108    0.201059\n",
       "12    0.085193  0.143549    0.393119    0.498694  0.768680    0.197950\n",
       "13    0.083288  0.154772    0.392255    0.487045  0.767614    0.204334\n",
       "14    0.082492  0.152486    0.407379    0.501995  0.762854    0.204372\n",
       "15    0.082388  0.138947    0.395833    0.492218  0.765939    0.205163\n",
       "16    0.121694  0.160455    0.506137    0.507255  0.752281    0.214745\n",
       "17    0.083445  0.145240    0.390019    0.499667  0.768146    0.195207\n",
       "18    0.083848  0.143468    0.390914    0.493470  0.758340    0.203117\n",
       "19    0.088031  0.152013    0.370210    0.497361  0.749725    0.203368\n",
       "20    0.086992  0.155779    0.383868    0.494335  0.758061    0.196931\n",
       "21    0.081623  0.142953    0.404795    0.488019  0.764454    0.201981\n",
       "22    0.083854  0.139906    0.385254    0.494115  0.761095    0.203807\n",
       "23    0.082174  0.158297    0.393931    0.485424  0.758052    0.205951\n",
       "24    0.082964  0.143239    0.386755    0.495257  0.757435    0.201933\n",
       "25    0.082129  0.142481    0.398661    0.485851  0.764020    0.204266\n",
       "26    0.082964  0.143239    0.386755    0.495257  0.757435    0.201933\n",
       "27    0.081407  0.141287    0.406771    0.491778  0.764096    0.205929\n",
       "28    0.082819  0.146104    0.391865    0.490361  0.758291    0.204744\n",
       "29    0.080485  0.140795    0.407174    0.489845  0.771014    0.204740\n",
       "..         ...       ...         ...         ...       ...         ...\n",
       "274   0.083365  0.160997    0.398397    0.487188  0.753722    0.206966\n",
       "275   0.085054  0.139094    0.428869    0.485480  0.762975    0.218413\n",
       "276   0.085742  0.149268    0.386235    0.493035  0.758248    0.196935\n",
       "277   0.082176  0.147220    0.396642    0.491838  0.761352    0.204333\n",
       "278   0.083432  0.145974    0.394160    0.488314  0.756478    0.206144\n",
       "279   0.085054  0.139094    0.428869    0.485480  0.762975    0.218413\n",
       "280   0.085742  0.149268    0.386235    0.493035  0.758248    0.196935\n",
       "281   0.082176  0.147220    0.396642    0.491838  0.761352    0.204333\n",
       "282   0.081701  0.151798    0.392981    0.488130  0.757891    0.206170\n",
       "283   0.085071  0.148486    0.383551    0.498654  0.761213    0.198458\n",
       "284   0.083775  0.135065    0.409145    0.490356  0.765941    0.204781\n",
       "285   0.084965  0.143742    0.386896    0.496411  0.761956    0.199775\n",
       "286   0.083775  0.135065    0.409145    0.490356  0.765941    0.204781\n",
       "287   0.082319  0.142386    0.397918    0.489065  0.762668    0.202005\n",
       "288   0.084283  0.141584    0.428508    0.484331  0.760022    0.210793\n",
       "289   0.084981  0.130835    0.412155    0.489763  0.765727    0.208824\n",
       "290   0.083710  0.144262    0.403361    0.490503  0.765894    0.201086\n",
       "291   0.083653  0.141793    0.399337    0.491239  0.760314    0.206270\n",
       "292   0.081469  0.141353    0.400527    0.488288  0.761137    0.204321\n",
       "293   0.083346  0.145944    0.392543    0.488315  0.759542    0.204455\n",
       "294   0.086446  0.142355    0.419190    0.491795  0.763711    0.208295\n",
       "295   0.084213  0.147191    0.400556    0.495741  0.764531    0.202379\n",
       "296   0.084629  0.147084    0.381661    0.499022  0.763895    0.192599\n",
       "297   0.083610  0.146985    0.415901    0.491329  0.757547    0.208384\n",
       "298   0.083107  0.146925    0.399797    0.489686  0.762292    0.207821\n",
       "299   0.085756  0.138976    0.398018    0.494349  0.763734    0.199436\n",
       "300   0.083107  0.146925    0.399797    0.489686  0.762292    0.207821\n",
       "301   0.085986  0.137841    0.390230    0.496191  0.761626    0.204471\n",
       "302   0.083858  0.137943    0.401072    0.489539  0.763566    0.202326\n",
       "303   0.084990  0.144653    0.384612    0.496984  0.761131    0.202513\n",
       "\n",
       "[304 rows x 6 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Book#</th>\n",
       "      <th>Description</th>\n",
       "      <th>economics</th>\n",
       "      <th>history</th>\n",
       "      <th>philosophy</th>\n",
       "      <th>psychology</th>\n",
       "      <th>science</th>\n",
       "      <th>technology</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>61</td>\n",
       "      <td>A Turing Award-winning computer scientist and ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60</td>\n",
       "      <td>A guide to understanding the inner workings a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59</td>\n",
       "      <td>100,000 years ago, at least six human species ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58</td>\n",
       "      <td>Since Alexis de Tocqueville, restlessness has ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>The New York Times bestselling book coauthored...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56</td>\n",
       "      <td>From Facebook's COO and Wharton's top-rated pr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>55</td>\n",
       "      <td>We often think the key to success and satisfac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>54</td>\n",
       "      <td>Much of the advice weâ€™ve been told about achie...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>53</td>\n",
       "      <td>A leading psychologist examines how our popula...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>52</td>\n",
       "      <td>The latest groundbreaking tome from Tim Ferris...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>51</td>\n",
       "      <td>From New York Times bestselling author and eco...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>50</td>\n",
       "      <td>What is the nature of space and time? How do w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>49</td>\n",
       "      <td>NATIONAL BESTSELLER CNBC and Strategy + Busine...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>48</td>\n",
       "      <td>In a tech-dominated world, the most needed deg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>47</td>\n",
       "      <td>In this pioneering examination of the scientif...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>46</td>\n",
       "      <td>The Courage to Be Disliked, already an enormou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>45</td>\n",
       "      <td>How do you get to what's real?Your organizatio...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>44</td>\n",
       "      <td>An essential book that unlocks the secrets of ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>43</td>\n",
       "      <td>A few common principles drive performance, reg...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>42</td>\n",
       "      <td>A Financial Times \"Business Book of the Month\"...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>41</td>\n",
       "      <td>Everyone knows that timing is everything. But ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>40</td>\n",
       "      <td>A fascinating exploration of how insights from...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>39</td>\n",
       "      <td>In this ground-breaking book perfect for reade...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>38</td>\n",
       "      <td>From the New York Times bestselling author of ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>37</td>\n",
       "      <td>Geoffrey West's research centres on a quest t...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>36</td>\n",
       "      <td>Yuval Noah Harari, author of the critically-ac...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>35</td>\n",
       "      <td>Foreword by Steven PinkerBlending the informed...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>34</td>\n",
       "      <td>Our Mathematical Universe is a journey to expl...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>33</td>\n",
       "      <td>Soccermatics]</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32</td>\n",
       "      <td>Human beings are primates, and primates are po...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>In this profound and lyrical book, one of our ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>29</td>\n",
       "      <td>From one of Americaâ€™s greatest minds, a journe...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>28</td>\n",
       "      <td>A lively history seen through the fifty invent...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>27</td>\n",
       "      <td>With his trademark blend of political history,...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>26</td>\n",
       "      <td>How will Artificial Intelligence affect crime,...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>25</td>\n",
       "      <td>How both logical and emotional reasoning can h...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>24</td>\n",
       "      <td>In this landmark book, Scott Page redefines th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>23</td>\n",
       "      <td>How humans and technology evolve together in a...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>22</td>\n",
       "      <td>Since the end of the second World War, economi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>21</td>\n",
       "      <td>It is widely understood that Charles Darwin's ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>20</td>\n",
       "      <td>An introduction to computational thinking that...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>19</td>\n",
       "      <td>Including a chapter by 2014 Nobel laureates Ma...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>18</td>\n",
       "      <td>â€œTimeâ€ is the most commonly used noun in the E...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17</td>\n",
       "      <td>The first full-scale history of cognitive scie...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>16</td>\n",
       "      <td>The Executive Brain is the first book to explo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>15</td>\n",
       "      <td>You are a mind reader, born with an extraordin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>14</td>\n",
       "      <td>How is it that thoroughly physical material be...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>13</td>\n",
       "      <td>A timely and uniquely compelling plea for the ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>12</td>\n",
       "      <td>Why do we think, feel, and act in ways we wish...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>11</td>\n",
       "      <td>The U.S. dollarâ€™s dominance seems under threat...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>10</td>\n",
       "      <td>With a 4-page full-color insert, and black-and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>9</td>\n",
       "      <td>A new consensus is emerging among cognitive sc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>8</td>\n",
       "      <td>Nerves make us bomb job interviews, first date...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>7</td>\n",
       "      <td>What is the mind? What is the experience of th...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>6</td>\n",
       "      <td>A unique trait of the human species is that ou...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>5</td>\n",
       "      <td>An award-winning scientist offers a groundbrea...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>4</td>\n",
       "      <td>Humans have built hugely complex societies and...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3</td>\n",
       "      <td>Why do some parents refuse to vaccinate their ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>Dr. John Bargh, the worldâ€™s leading expert on ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>A new theory of how the brain constructs emoti...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Book#                                        Description  economics  \\\n",
       "0      61  A Turing Award-winning computer scientist and ...        0.0   \n",
       "1      60   A guide to understanding the inner workings a...        0.0   \n",
       "2      59  100,000 years ago, at least six human species ...        0.0   \n",
       "3      58  Since Alexis de Tocqueville, restlessness has ...        1.0   \n",
       "4      57  The New York Times bestselling book coauthored...        0.0   \n",
       "5      56  From Facebook's COO and Wharton's top-rated pr...        0.0   \n",
       "6      55  We often think the key to success and satisfac...        0.0   \n",
       "7      54  Much of the advice weâ€™ve been told about achie...        0.0   \n",
       "8      53  A leading psychologist examines how our popula...        0.0   \n",
       "9      52  The latest groundbreaking tome from Tim Ferris...        0.0   \n",
       "10     51  From New York Times bestselling author and eco...        1.0   \n",
       "11     50  What is the nature of space and time? How do w...        0.0   \n",
       "12     49  NATIONAL BESTSELLER CNBC and Strategy + Busine...        0.0   \n",
       "13     48  In a tech-dominated world, the most needed deg...        0.0   \n",
       "14     47  In this pioneering examination of the scientif...        0.0   \n",
       "15     46  The Courage to Be Disliked, already an enormou...        0.0   \n",
       "16     45  How do you get to what's real?Your organizatio...        0.0   \n",
       "17     44  An essential book that unlocks the secrets of ...        0.0   \n",
       "18     43  A few common principles drive performance, reg...        0.0   \n",
       "19     42  A Financial Times \"Business Book of the Month\"...        0.0   \n",
       "20     41  Everyone knows that timing is everything. But ...        0.0   \n",
       "21     40  A fascinating exploration of how insights from...        0.0   \n",
       "22     39  In this ground-breaking book perfect for reade...        0.0   \n",
       "23     38  From the New York Times bestselling author of ...        1.0   \n",
       "24     37   Geoffrey West's research centres on a quest t...        1.0   \n",
       "25     36  Yuval Noah Harari, author of the critically-ac...        0.0   \n",
       "26     35  Foreword by Steven PinkerBlending the informed...        1.0   \n",
       "27     34  Our Mathematical Universe is a journey to expl...        0.0   \n",
       "28     33                                      Soccermatics]        0.0   \n",
       "29     32  Human beings are primates, and primates are po...        1.0   \n",
       "..    ...                                                ...        ...   \n",
       "31     30  In this profound and lyrical book, one of our ...        0.0   \n",
       "32     29  From one of Americaâ€™s greatest minds, a journe...        0.0   \n",
       "33     28  A lively history seen through the fifty invent...        1.0   \n",
       "34     27  With his trademark blend of political history,...        1.0   \n",
       "35     26  How will Artificial Intelligence affect crime,...        0.0   \n",
       "36     25  How both logical and emotional reasoning can h...        0.0   \n",
       "37     24  In this landmark book, Scott Page redefines th...        0.0   \n",
       "38     23  How humans and technology evolve together in a...        0.0   \n",
       "39     22  Since the end of the second World War, economi...        1.0   \n",
       "40     21  It is widely understood that Charles Darwin's ...        0.0   \n",
       "41     20  An introduction to computational thinking that...        0.0   \n",
       "42     19  Including a chapter by 2014 Nobel laureates Ma...        0.0   \n",
       "43     18  â€œTimeâ€ is the most commonly used noun in the E...        0.0   \n",
       "44     17  The first full-scale history of cognitive scie...        0.0   \n",
       "45     16  The Executive Brain is the first book to explo...        0.0   \n",
       "46     15  You are a mind reader, born with an extraordin...        0.0   \n",
       "47     14  How is it that thoroughly physical material be...        0.0   \n",
       "48     13  A timely and uniquely compelling plea for the ...        0.0   \n",
       "49     12  Why do we think, feel, and act in ways we wish...        0.0   \n",
       "50     11  The U.S. dollarâ€™s dominance seems under threat...        1.0   \n",
       "51     10  With a 4-page full-color insert, and black-and...        0.0   \n",
       "52      9  A new consensus is emerging among cognitive sc...        0.0   \n",
       "53      8  Nerves make us bomb job interviews, first date...        0.0   \n",
       "54      7  What is the mind? What is the experience of th...        0.0   \n",
       "55      6  A unique trait of the human species is that ou...        0.0   \n",
       "56      5  An award-winning scientist offers a groundbrea...        0.0   \n",
       "57      4  Humans have built hugely complex societies and...        0.0   \n",
       "58      3  Why do some parents refuse to vaccinate their ...        0.0   \n",
       "59      2  Dr. John Bargh, the worldâ€™s leading expert on ...        0.0   \n",
       "60      1  A new theory of how the brain constructs emoti...        0.0   \n",
       "\n",
       "    history  philosophy  psychology  science  technology  \n",
       "0       0.0         1.0         0.0      1.0         0.0  \n",
       "1       0.0         0.0         0.0      0.0         1.0  \n",
       "2       1.0         1.0         0.0      1.0         0.0  \n",
       "3       1.0         0.0         0.0      0.0         0.0  \n",
       "4       0.0         0.0         1.0      1.0         0.0  \n",
       "5       0.0         0.0         1.0      0.0         0.0  \n",
       "6       0.0         0.0         1.0      0.0         0.0  \n",
       "7       0.0         0.0         1.0      1.0         0.0  \n",
       "8       0.0         0.0         1.0      0.0         0.0  \n",
       "9       0.0         0.0         0.0      0.0         0.0  \n",
       "10      0.0         1.0         1.0      0.0         0.0  \n",
       "11      0.0         0.0         0.0      1.0         0.0  \n",
       "12      0.0         0.0         1.0      1.0         1.0  \n",
       "13      0.0         0.0         0.0      0.0         0.0  \n",
       "14      0.0         0.0         1.0      1.0         0.0  \n",
       "15      0.0         1.0         1.0      0.0         0.0  \n",
       "16      0.0         0.0         1.0      0.0         0.0  \n",
       "17      0.0         0.0         1.0      0.0         0.0  \n",
       "18      0.0         0.0         1.0      0.0         0.0  \n",
       "19      0.0         1.0         1.0      1.0         1.0  \n",
       "20      0.0         0.0         1.0      1.0         0.0  \n",
       "21      0.0         0.0         1.0      1.0         1.0  \n",
       "22      0.0         0.0         1.0      1.0         0.0  \n",
       "23      0.0         1.0         1.0      0.0         0.0  \n",
       "24      0.0         0.0         0.0      1.0         0.0  \n",
       "25      1.0         1.0         0.0      1.0         0.0  \n",
       "26      0.0         0.0         1.0      1.0         1.0  \n",
       "27      0.0         1.0         0.0      1.0         0.0  \n",
       "28      0.0         0.0         0.0      1.0         0.0  \n",
       "29      0.0         1.0         1.0      1.0         0.0  \n",
       "..      ...         ...         ...      ...         ...  \n",
       "31      1.0         1.0         1.0      1.0         0.0  \n",
       "32      0.0         1.0         1.0      1.0         0.0  \n",
       "33      1.0         0.0         0.0      1.0         1.0  \n",
       "34      1.0         1.0         0.0      0.0         0.0  \n",
       "35      0.0         1.0         0.0      1.0         1.0  \n",
       "36      0.0         1.0         0.0      1.0         0.0  \n",
       "37      0.0         0.0         1.0      0.0         0.0  \n",
       "38      0.0         1.0         0.0      1.0         1.0  \n",
       "39      0.0         0.0         0.0      0.0         0.0  \n",
       "40      1.0         1.0         0.0      1.0         0.0  \n",
       "41      0.0         0.0         0.0      0.0         0.0  \n",
       "42      0.0         0.0         1.0      1.0         0.0  \n",
       "43      0.0         1.0         1.0      1.0         0.0  \n",
       "44      1.0         1.0         1.0      1.0         0.0  \n",
       "45      0.0         0.0         1.0      1.0         0.0  \n",
       "46      0.0         0.0         1.0      1.0         0.0  \n",
       "47      0.0         1.0         1.0      1.0         0.0  \n",
       "48      0.0         1.0         1.0      1.0         0.0  \n",
       "49      0.0         0.0         1.0      1.0         0.0  \n",
       "50      0.0         0.0         0.0      0.0         0.0  \n",
       "51      0.0         0.0         1.0      1.0         0.0  \n",
       "52      0.0         0.0         1.0      1.0         0.0  \n",
       "53      0.0         0.0         1.0      1.0         0.0  \n",
       "54      0.0         1.0         1.0      1.0         0.0  \n",
       "55      1.0         0.0         1.0      1.0         0.0  \n",
       "56      0.0         0.0         1.0      1.0         0.0  \n",
       "57      0.0         1.0         1.0      1.0         0.0  \n",
       "58      0.0         0.0         1.0      1.0         0.0  \n",
       "59      0.0         0.0         1.0      1.0         0.0  \n",
       "60      0.0         0.0         1.0      1.0         0.0  \n",
       "\n",
       "[61 rows x 8 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p27)",
   "language": "python",
   "name": "conda_tensorflow_p27"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
